# Tooling & Environment Setup

> **Infrastructure and tools needed for effective AI-DLC adoption.**

## The AI-DLC Tooling Stack

AI-DLC requires specific tooling categories. You don't need all of these to start, but you'll eventually want each layer.

```mermaid
flowchart TB
    subgraph Layer1["ðŸ—ï¸ Layer 1: Foundation"]
        direction TB
        L1A[ðŸ¤– AI Coding Assistant]
        L1B[ðŸ“š Version Control]
        L1C[ðŸ”„ CI/CD Pipeline]
    end

    subgraph Layer2["âœ… Layer 2: Quality"]
        direction TB
        L2A[ðŸ§ª Automated Testing]
        L2B[ðŸ” Static Analysis]
        L2C[ðŸ”’ Security Scanning]
    end

    subgraph Layer3["ðŸ“‹ Layer 3: Process"]
        direction TB
        L3A[ðŸ“Š Unit Tracking]
        L3B[ðŸ“ Criteria Management]
        L3C[ðŸ“ˆ Metrics Dashboard]
    end

    subgraph Layer4["ðŸš€ Layer 4: Advanced"]
        direction TB
        L4A[ðŸ§  Context Management]
        L4B[ðŸ“– Knowledge Base]
        L4C[ðŸŽ¯ AI Orchestration]
    end

    Layer1 --> Layer2
    Layer2 --> Layer3
    Layer3 --> Layer4

    style Layer1 fill:#c8e6c9
    style Layer2 fill:#c8e6c9
    style Layer3 fill:#fff3e0
    style Layer4 fill:#e1f5fe
```

## Layer 1: Foundation (Required)

### AI Coding Assistant

The core tool for AI-DLC. Options:

| Tool | Strengths | Considerations |
|------|-----------|----------------|
| Claude Code | Deep reasoning, long context | Subscription cost |
| GitHub Copilot | IDE integration, code completion | Less autonomous |
| Cursor | AI-native IDE | Learning curve |
| Cody | Enterprise features | Self-hosted option |

**Minimum requirements:**

- Multi-file context awareness
- Ability to execute code/tests
- Conversational interface for elaboration

### Version Control Configuration

Git configuration for AI-DLC workflows:

```bash
# Recommended .gitignore additions
.ai-context/
.claude/cache/
*.ai-generated.md

# Branch naming convention
# feature/unit-{unit-id}-{brief-description}
# e.g., feature/unit-42-add-user-export
```

**Commit conventions:**

```
feat(unit-42): Add CSV export functionality

Criteria met:
- [x] Exports name, email, created_at columns
- [x] Handles 10k+ records without timeout
- [x] UTF-8 encoding for international names

Mode: Observed
AI-assisted: Yes
```

### CI/CD Pipeline Enhancements

Add AI-DLC quality gates:

```yaml
# .github/workflows/ai-dlc-quality.yml
name: AI-DLC Quality Gates

on: [push, pull_request]

jobs:
  criteria-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run tests
        run: npm test

      - name: Type check
        run: npm run typecheck

      - name: Lint
        run: npm run lint

      - name: Security scan
        run: npm run security-scan

      - name: Coverage threshold
        run: npm run coverage -- --threshold=80
```

## Layer 2: Quality (Required)

### Automated Testing Setup

Testing is the primary backpressure mechanism in AI-DLC.

```mermaid
flowchart TB
    subgraph Testing["ðŸ§ª Test Types"]
        direction TB
        T1[ðŸ”¬ Unit Tests]
        T2[ðŸ”— Integration Tests]
        T3[ðŸŽ­ E2E Tests]
    end

    Testing --> Trigger

    subgraph Trigger["â° When They Run"]
        direction TB
        TR1[ðŸ’¾ Save/commit]
        TR2[ðŸ“¤ Push]
        TR3[ðŸ”€ Pre-merge]
    end

    style Testing fill:#c8e6c9
```

**Test configuration for fast feedback:**

```json
{
  "scripts": {
    "test": "vitest run",
    "test:watch": "vitest",
    "test:coverage": "vitest run --coverage",
    "test:changed": "vitest run --changed"
  }
}
```

### Static Analysis

Catches issues before tests run:

| Tool | Purpose | Configuration |
|------|---------|---------------|
| ESLint/Biome | Code style, patterns | Strict ruleset |
| TypeScript | Type safety | `strict: true` |
| Prettier/Biome | Formatting | Auto-format on save |

**Biome configuration for AI-DLC:**

```json
{
  "linter": {
    "enabled": true,
    "rules": {
      "recommended": true,
      "suspicious": {
        "noExplicitAny": "error"
      }
    }
  },
  "formatter": {
    "enabled": true
  }
}
```

### Security Scanning

Non-negotiable for AI-generated code:

| Tool | Checks | Integration |
|------|--------|-------------|
| Snyk | Dependencies, code | CI/CD, IDE |
| Semgrep | Code patterns | Pre-commit, CI |
| GitGuardian | Secrets | Pre-commit |
| npm audit | Dependencies | CI/CD |

```yaml
# Pre-commit security hooks
repos:
  - repo: https://github.com/Yelp/detect-secrets
    rev: v1.4.0
    hooks:
      - id: detect-secrets
```

## Layer 3: Process (Recommended)

### Unit Tracking

Track Units through their lifecycle:

```mermaid
flowchart TB
    subgraph States["ðŸ“Š Unit States"]
        direction TB
        I[ðŸ’¡ Intent]
        I --> E[ðŸ’¬ Elaboration]
        E --> C[ðŸ”¨ Construction]
        C --> V[âœ… Verification]
        V --> D[ðŸŽ‰ Done]
    end

    style I fill:#e1f5fe
    style E fill:#fff3e0
    style C fill:#fff3e0
    style V fill:#fff3e0
    style D fill:#c8e6c9
```

**Options:**

- Jira with custom fields for mode, criteria
- Linear with AI-DLC workflow
- GitHub Projects with automation
- Custom tracking in your codebase

### Criteria Management

Store and version completion criteria:

```markdown
<!-- units/unit-42-csv-export.md -->
# Unit 42: CSV Export

## Intent
Users can export their data as CSV for backup and analysis.

## Completion Criteria
- [ ] Export includes: name, email, created_at
- [ ] Handles 10,000+ records in <5 seconds
- [ ] UTF-8 encoding with BOM for Excel compatibility
- [ ] Progress indicator for large exports
- [ ] Error handling with user-friendly messages

## Mode Selection
- Initial: Observed (moderate complexity)
- Can graduate to: Autonomous after first success

## Context
- Related code: `src/services/export/`
- Similar feature: PDF export in `src/services/pdf/`
- Constraints: Must work in Safari (no Web Workers)
```

### Metrics Dashboard

See [Metrics & Measurement](/papers/ai-dlc-2026/runbooks/metrics-measurement) for detailed dashboard design.

Quick setup with existing tools:

- **Grafana** â€” If you have existing observability
- **Notion/Coda** â€” Simple dashboards with formulas
- **Google Sheets** â€” Lightweight, accessible

## Layer 4: Advanced (Optional)

### Context Management

For larger codebases, manage what AI sees:

```mermaid
flowchart TB
    subgraph Context["ðŸ§  Context Layers"]
        direction TB
        C1[ðŸ“ Project conventions]
        C2[ðŸ“‚ Relevant code files]
        C3[ðŸ“‹ Current Unit criteria]
        C4[âš ï¸ Error history]
    end

    subgraph Tools["ðŸ› ï¸ Context Tools"]
        direction TB
        T1[ðŸ“„ CLAUDE.md files]
        T2[ðŸ—‚ï¸ AI context files]
        T3[ðŸ“š Knowledge bases]
    end

    Context --> Tools

    style Context fill:#e1f5fe
    style Tools fill:#c8e6c9
```

**CLAUDE.md structure:**

```markdown
# Project Context for AI

## Architecture
- Monorepo with packages/ structure
- React frontend, Node.js backend
- PostgreSQL database

## Conventions
- Use functional components with hooks
- Prefer Zod for validation
- All API endpoints in /api/v1/

## Patterns to Follow
- See src/services/user/ for service pattern
- See src/components/Button/ for component pattern

## Anti-Patterns to Avoid
- No class components
- No any types
- No console.log in production code
```

### Knowledge Base

Capture learnings for future AI interactions:

| Content Type | Purpose | Update Frequency |
|--------------|---------|-----------------|
| Prompt patterns | What works | After each Unit |
| Failure patterns | What doesn't | After each failure |
| Criteria templates | Starting points | Monthly |
| Mode selection guide | Decision support | Quarterly |

### AI Orchestration

For complex workflows with multiple AI interactions:

```mermaid
flowchart TB
    subgraph Orchestration["ðŸŽ¯ Orchestration Patterns"]
        direction TB
        O1[âž¡ï¸ Sequential: A â†’ B â†’ C]
        O2[âš¡ Parallel: A + B + C]
        O3[ðŸ”€ Conditional: If X then A else B]
    end

    style Orchestration fill:#e1f5fe
```

Tools: Han (this project), LangChain, AutoGen

## Minimum Viable Setup

Start with this and expand:

```
âœ… AI Coding Assistant (Claude Code, Copilot, etc.)
âœ… Git with good .gitignore
âœ… CI/CD with test + lint + typecheck
âœ… Security scanning (at minimum, dependency audit)
âœ… Simple unit tracking (even a markdown file)
```

## Environment Configuration

### IDE Setup

```json
// VS Code settings.json for AI-DLC
{
  "editor.formatOnSave": true,
  "editor.codeActionsOnSave": {
    "source.fixAll": true
  },
  "typescript.tsdk": "node_modules/typescript/lib",
  "files.exclude": {
    "**/.ai-context": true
  }
}
```

### Shell Configuration

```bash
# .zshrc / .bashrc additions
export CLAUDE_PROJECT_ROOT=$(git rev-parse --show-toplevel 2>/dev/null)

# Aliases for common AI-DLC commands
alias aitest="npm run test:changed"
alias ailint="npm run lint -- --fix"
alias aicheck="npm run typecheck && npm run lint && npm test"
```

## Entry Criteria

- Team has selected an AI coding assistant
- Existing CI/CD pipeline in place
- Basic test suite exists

## Exit Criteria

- [ ] AI coding assistant configured for all developers
- [ ] CI/CD pipeline includes quality gates
- [ ] Security scanning in place
- [ ] Unit tracking system operational
- [ ] CLAUDE.md or equivalent context file created
- [ ] Team trained on tooling usage

## Common Failure Modes

### 1. Over-Engineering the Stack

**Symptom:** Spending weeks on perfect tooling before writing any code.

**Fix:** Start with minimum viable setup. Add layers as pain points emerge.

### 2. Ignoring Security Scanning

**Symptom:** AI-generated code with vulnerabilities reaches production.

**Fix:** Security scanning is Layer 2, not Layer 4. Add it early.

### 3. No Context Management

**Symptom:** AI repeatedly makes same mistakes, ignores conventions.

**Fix:** Create CLAUDE.md immediately. Update it as you learn.

### 4. Manual Quality Gates

**Symptom:** Relying on humans to catch what tools should catch.

**Fix:** If it can be automated, automate it. Save humans for judgment calls.

## Related Runbooks

- [Quality Guardrails](/papers/ai-dlc-2026/runbooks/quality-guardrails) â€” Quality-specific tooling
- [Incremental Adoption](/papers/ai-dlc-2026/runbooks/incremental-adoption) â€” When to add each layer
- [Metrics & Measurement](/papers/ai-dlc-2026/runbooks/metrics-measurement) â€” Dashboard setup
- [Overcoming Tech Debt](/papers/ai-dlc-2026/runbooks/overcoming-tech-debt) â€” Starting with limited infrastructure
