{
    "version": "https://jsonfeed.org/version/1",
    "title": "Han Blog - Claude Code Plugin Marketplace",
    "home_page_url": "https://han.guru",
    "feed_url": "https://han.guru/feed.json",
    "description": "News, tutorials, and insights about Han plugins for Claude Code built on Bushido principles",
    "icon": "https://han.guru/og-image.png",
    "author": {
        "name": "The Bushido Collective",
        "url": "https://han.guru"
    },
    "items": [
        {
            "id": "https://han.guru/blog/ai-dlc-2026-paper",
            "content_html": "\nSoftware development has entered a new era. AI agents can now sustain multi-hour reasoning sessions, write thousands of lines of production code, and iterate toward success criteria with minimal human intervention. But our development methodologies haven't caught up.\n\nToday, we're publishing **[AI-Driven Development Lifecycle 2026 (AI-DLC 2026)](/papers/ai-dlc-2026)** — a comprehensive methodology reimagined from first principles for the age of autonomous agents.\n\n## The Core Insight: From Human-in-the-Loop to Human-on-the-Loop\n\nTraditional \"human-in-the-loop\" (HITL) workflows require humans to validate every AI decision before proceeding. This made sense when AI was unreliable. But frontier models can now complete tasks that take humans multiple hours, and independent production deployments regularly write tens of thousands of lines of code monthly.\n\nAI-DLC 2026 introduces **human-on-the-loop (HOTL)** as a distinct operating mode:\n\n- **HITL**: Human validates each step. AI proposes, human approves, AI executes.\n- **HOTL**: Human defines success criteria. AI iterates autonomously until criteria are met.\n\nThink of it like Google Maps navigation:\n- HITL mode: You approve every turn before the GPS proceeds\n- HOTL mode: You set the destination and constraints, GPS handles the journey\n\nThe key: humans don't disappear. Their function changes from micromanaging execution to defining outcomes and building quality gates.\n\n## Backpressure Over Prescription\n\nInstead of prescribing *how* AI should work (\"first write the interface, then implement the class, then write unit tests\"), AI-DLC 2026 defines *what* must be satisfied:\n\n- All tests must pass\n- Type checks must succeed\n- Linting must be clean\n- Security scans must clear\n- Coverage must exceed threshold\n\nLet AI determine how to satisfy these constraints. Each failure provides signal. Each iteration refines the approach.\n\n> \"Better to fail predictably than succeed unpredictably.\"\n> — Geoffrey Huntley\n\n## The Ralph Wiggum Pattern\n\nNamed after the Simpsons character, this autonomous loop pattern embraces \"deterministically bad in an undeterministic world.\" Rather than trying to be perfect, the agent:\n\n1. Tries an approach\n2. Runs quality gates\n3. Learns from failures\n4. Iterates until success criteria are met\n5. Outputs `COMPLETE` or `BLOCKED`\n\nProduction systems using this pattern have achieved remarkable results — 40,000+ lines written by AI using AI in a single month.\n\n## The Collapsing SDLC\n\nTraditional phase boundaries — requirements → design → implementation → testing → deployment — existed because iteration was expensive. When changing requirements meant weeks of rework, sequential phases with approval gates made economic sense.\n\nWith AI, iteration costs approach zero. You try something, it fails, you adjust, you try again — all in seconds, not weeks.\n\n**The phases aren't being augmented. They're collapsing into continuous flow.**\n\nCheckpoints replace handoffs:\n- Work pauses briefly rather than stopping completely\n- The same agent continues with feedback\n- Context is preserved\n- Git and files carry knowledge\n\n## What's Inside\n\nThe full methodology covers:\n\n- **10 Core Principles**: From reimagining rather than retrofitting to embracing memory providers\n- **Artifacts & Phases**: Intents, Units, Bolts, and how they flow through Inception, Construction, and Operations\n- **Decision Framework**: When to use supervised vs. autonomous modes\n- **Implementation Patterns**: Prompt templates, quality gate configuration, file-based memory\n- **Real Examples**: Greenfield and brownfield development scenarios\n- **Adoption Path**: How teams can transition incrementally\n\n## Built on Foundational Work\n\nAI-DLC 2026 synthesizes insights from:\n\n- **Raja SP** (AWS): Original AI-DLC methodology and core concepts\n- **Geoffrey Huntley**: Ralph Wiggum pattern and autonomous loop philosophy\n- **Boris Cherny & Anthropic**: Ralph Wiggum plugin demonstrating production viability\n- **Steve Wilson** (OWASP): Human-on-the-loop governance frameworks\n- **paddo.dev**: Analysis of SDLC collapse and multi-agent orchestration pitfalls\n- **HumanLayer**: 12 Factor Agents and context engineering research\n\n## Read the Full Paper\n\nThis is just a glimpse. The full methodology includes:\n\n- Detailed workflows and rituals\n- Mob Elaboration for collaborative requirements gathering\n- Autonomous Bolt templates and safety configurations\n- Memory layer architecture\n- Metrics evolution for AI-driven teams\n- Complete decision trees and quick reference guides\n\n**[Read AI-DLC 2026 →](/papers/ai-dlc-2026)**\n\n## Why This Matters for Han\n\nHan embodies many AI-DLC 2026 principles:\n\n- **Backpressure through hooks**: Quality gates that automatically validate work\n- **Autonomous validation**: Stop hooks run without human intervention\n- **File-based memory**: Project rules persist across sessions\n- **Completion criteria**: Plugins define measurable success conditions\n\nThe methodology provides the theoretical foundation. Han provides the practical implementation.\n\n---\n\n*AI-DLC 2026 is an open methodology. We welcome contributions, adaptations, and real-world feedback as teams put these principles into practice.*\n",
            "url": "https://han.guru/blog/ai-dlc-2026-paper",
            "title": "Introducing AI-DLC 2026: A Methodology for Autonomous Development",
            "summary": "We're publishing a comprehensive methodology for AI-driven software development, introducing human-on-the-loop workflows, backpressure-driven quality, and autonomous development loops.",
            "date_modified": "2026-01-21T00:00:00.000Z",
            "author": {
                "name": "The Bushido Collective"
            },
            "tags": [
                "Research",
                "methodology",
                "autonomous-agents",
                "ai-development",
                "research",
                "hotl"
            ]
        },
        {
            "id": "https://han.guru/blog/han-memory-system",
            "content_html": "\nOur [previous post on project memory](/blog/project-memory-feature) introduced Han's `learn` tool - Claude autonomously capturing knowledge to `.claude/rules/`. That was step one.\n\nBut real memory is more than just writing things down. It's knowing what you discussed three sessions ago. It's finding the reasoning behind decisions. It's understanding who on your team has expertise in what. It's patterns emerging from practice across your entire organization.\n\nHan's Memory System delivers all five layers.\n\n## The Problem: Context Vanishes\n\nEvery developer has experienced this:\n\n- \"Why did we choose this approach?\" *No one remembers.*\n- \"Who built the payment system?\" *They left six months ago.*\n- \"What did we try last time this broke?\" *Lost in a Slack thread somewhere.*\n- \"What was I working on yesterday?\" *Scrolling through git log...*\n\nAI assistants make this worse. You have an incredible conversation with Claude - exploring tradeoffs, making decisions, building understanding. Then the session ends and it's gone. The next session starts fresh, asking questions you already answered.\n\n**Han fixes this.** Not with some abstract \"memory database\" but with five layers of context that mirror how teams actually work.\n\n## Five Layers of Context\n\n<ArchitectureDiagram />\n\nEach layer answers different questions at different speeds. Together, they give Claude - and you - complete context.\n\n## Layer 1: Rules - How We Do Things Here\n\nThe fastest layer. Project conventions that apply immediately:\n\n<Callout type=\"tip\" title=\"What Rules Capture\">\n\n- Coding standards specific to your project\n- Architectural decisions (\"always use X for Y\")\n- Testing conventions\n- Error handling patterns\n- Team agreements\n\n</Callout>\n\nRules live in `.claude/rules/` and are git-tracked. They're shared with your team, versioned, and reviewable. Unlike scattered documentation, they're injected directly into Claude's context - always present, always applied.\n\nRules can be written manually or auto-promoted from patterns Han observes in practice. Your conventions become documented naturally.\n\n## Layer 2: Session Summaries - What You've Been Doing\n\nWhen you end a session, Han summarizes what happened: what you worked on, what decisions were made, what's still in progress.\n\nThis isn't just a log. It's searchable context:\n\n<Terminal title=\"Picking Up Where You Left Off\">\n```\nYou: What was I working on yesterday?\n\nClaude: Yesterday's session (2.5 hours):\n\n- Implemented refresh token rotation\n- Fixed the race condition in logout\n- Started but didn't finish: rate limiting middleware\n\nYou left a note: \"Rate limiter needs Redis - check with ops first\"\n```\n</Terminal>\n\nNo more digging through git logs or trying to remember where you left off.\n\n## Layer 3: Observations - The Detailed Trail\n\nEvery tool use is logged: files read, files modified, commands run, tests executed. This creates a detailed trail of work without you doing anything.\n\nWhen summaries aren't enough, observations provide the detail:\n\n- \"What files did I touch related to auth?\"\n- \"Did I run the tests before committing?\"\n- \"What commands did I use to debug this?\"\n\nThis layer exists for when you need specifics, not just summaries.\n\n## Layer 4: Transcripts - The Reasoning\n\n<Callout type=\"info\" title=\"The Missing Link\">\nClaude Code stores full conversation history. Han can now search it.\n</Callout>\n\nThis is the game-changer. Because context lives in conversations:\n\n<Terminal title=\"Finding the 'Why'\">\n```\nYou: Why did we use JWT instead of sessions?\n\nClaude: Found in conversation from December 10th:\n\nYou discussed this with Claude for about 20 minutes. The key points:\n- API is stateless, needs to scale horizontally\n- Mobile clients need offline token validation\n- Microservices benefit from decentralized auth\n- You considered sessions but rejected for latency reasons\n\nThe decision was made with full context of your architecture.\n```\n</Terminal>\n\nThat reasoning was lost when sessions ended. Now it's searchable, forever.\n\n**Cross-worktree search** means context follows the repository, not just the directory. Working on a feature branch in a separate worktree? Han finds relevant context from all worktrees sharing the same repo.\n\n## Layer 5: Team Memory - Institutional Knowledge\n\nThis is where Han goes beyond personal memory. Your team's institutional knowledge exists - it's just scattered:\n\n<Callout type=\"note\" title=\"Team Knowledge Sources\">\n\n- **Git history**: Who changed what, when, and why (commit messages, PR descriptions)\n- **Issue trackers**: Decisions made in GitHub Issues, Linear, Jira\n- **Project management**: Context from ClickUp, Asana, Monday\n- **Communication**: Relevant discussions (where accessible)\n- **Documentation**: READMEs, wikis, architectural decision records\n\n</Callout>\n\nHan's team memory isn't just git. It's a research engine that connects to your team's actual knowledge sources through MCP integrations:\n\n<Terminal title=\"Finding Team Expertise\">\n```\nYou: Who should I talk to about the payment system?\n\nClaude: Based on team memory research:\n\n**Primary expert: Sarah Chen** (@schen)\n- 47 commits touching payments/ (highest contributor)\n- Authored PR #234 \"Stripe integration\"\n- Resolved Issues #89, #112, #156 (all payment-related)\n\n**Key context found:**\n- Linear issue LIN-234: \"Chose Stripe over Braintree for international\"\n- GitHub discussion: \"Idempotency keys added after incident\"\n- Last significant change: 3 weeks ago\n\nSarah would be your best starting point.\n```\n</Terminal>\n\nEvidence-based answers. Not just \"ask someone\" but \"ask this specific person, here's why.\"\n\n## Practical Impact\n\n<Callout type=\"tip\" title=\"What This Actually Means\">\n\n**Onboarding acceleration**: New team members get context immediately. \"Who owns this?\" and \"Why is it this way?\" have answers.\n\n**Decision archaeology**: Find not just what was decided, but the full discussion. Understand constraints that existed at the time.\n\n**Expertise mapping**: Know who to ask without bothering everyone. See contribution patterns, not just org charts.\n\n**Session continuity**: Pick up exactly where you left off. Context survives session boundaries.\n\n**Emergent documentation**: Rules reflect actual practice, not aspirational guidelines. Your conventions document themselves.\n\n</Callout>\n\n## The Unified Query\n\nOne interface handles all memory questions:\n\n<Terminal title=\"Natural Memory Queries\">\n```\n\"What was I working on?\"\n→ Your recent sessions and observations\n\n\"What did we discuss about auth?\"\n→ Full conversation transcripts\n\n\"Who knows about payments?\"\n→ Team expertise and git history\n\n\"How do we handle errors here?\"\n→ Project rules and conventions\n```\n</Terminal>\n\nClaude determines the question type and searches appropriate layers. You don't think about layers - you just ask questions.\n\n## Getting Started\n\nHan's memory system activates with the core plugin:\n\n```bash\nhan plugin install core\n```\n\nEverything happens automatically:\n\n- Observations captured as you work\n- Sessions summarized when you stop\n- Context injected when you start\n\nFor team memory integrations (GitHub Issues, Linear, etc.), add the relevant hashi plugins:\n\n```bash\nhan plugin install hashi-github\nhan plugin install hashi-linear\n```\n\n---\n\n<Callout type=\"info\">\nMemory isn't a feature. It's how teams actually work - building on each other's knowledge, learning from past decisions, knowing who to ask. Han makes that work for AI assistance too.\n</Callout>\n",
            "url": "https://han.guru/blog/han-memory-system",
            "title": "Han's Five-Layer Memory System: Context That Matters",
            "summary": "How Han transforms fleeting conversations into lasting institutional knowledge - connecting personal work history, team expertise, and project wisdom.",
            "date_modified": "2025-12-13T00:00:00.000Z",
            "author": {
                "name": "The Bushido Collective"
            },
            "tags": [
                "Features",
                "memory",
                "research",
                "mcp",
                "learning",
                "team",
                "context"
            ]
        },
        {
            "id": "https://han.guru/blog/checkpoint-system",
            "content_html": "\nEarly Han was aggressive. Maybe too aggressive.\n\nWhen you installed a jutsu plugin with validation hooks, Han went scorched earth. It found every lint error, every type issue, every test failure across your entire codebase. On the first session. Before you'd written a single line of code.\n\nFor maintainers of pristine codebases, this was fine. For everyone else - which is most of us - it was overwhelming. You'd install a plugin hoping for helpful validation, and immediately face hundreds of issues you didn't create and weren't ready to fix.\n\nNew adopters bounced. They disabled hooks or uninstalled plugins. The validation that was supposed to help became an obstacle.\n\n## The Boy Scout Rule\n\nThe Boy Scout Rule says: leave the campground cleaner than you found it. Not pristine. Not spotless. Just *better*.\n\nHan's new checkpoint system applies this principle to validation. Instead of identifying every problem in your codebase, hooks now focus on **the code you actually touched**. Work in a directory? Improve that directory. Leave the rest for later.\n\n## How Checkpoints Work\n\nWhen a Claude Code session starts, Han captures a checkpoint - a snapshot of file states at that moment. When hooks run at session end, they filter using this checkpoint.\n\n**A file is only validated if:**\n\n- It changed since the checkpoint (you modified it), AND\n- It changed since the last hook run\n\nThis intersection ensures hooks analyze your actual work. Pre-existing issues in files you never opened are out of scope.\n\n```\nSession Start (t0):\n├─ Checkpoint created\n├─ File hashes captured for all project files\n\nYour Work (t0 → t1):\n├─ Modified: components/Button.tsx\n├─ Untouched: utils/format.ts (has lint errors)\n\nSession Stop (t1):\n├─ Hook runs\n├─ Filters to: components/Button.tsx only\n└─ Pre-existing issues in utils/format.ts: not your problem today\n```\n\n## Incremental Improvement\n\nThe old approach had a certain logic: surface all problems so you know what exists. But in practice, this created paralysis:\n\n- Developers felt blamed for issues they didn't cause\n- The sheer volume made prioritization impossible\n- Hooks got disabled, eliminating all validation\n- Net result: worse quality, not better\n\nThe Boy Scout approach acknowledges reality: most codebases have accumulated issues. You can't fix everything at once, and demanding that creates learned helplessness.\n\nInstead, checkpoint-based validation creates a sustainable path:\n\n1. You touch a file\n2. That file gets validated\n3. You fix issues *in the code you were already changing*\n4. Over time, frequently-touched code gets cleaner\n5. Rarely-touched code stays as-is until relevant\n\n## Technical Implementation\n\nCheckpoints live in `~/.claude/projects/{slug}/han/checkpoints/`:\n\n**Session Checkpoints**: `session_{session_id}.json`\n\n- Created on SessionStart\n- Contains SHA-256 hashes of all files\n- Cleaned up after 24 hours\n\n**Agent Checkpoints**: `agent_{agent_id}.json`\n\n- Created for subagents (separate Claude instances)\n- Scopes validation to each agent's work\n- Prevents cross-contamination between parallel work\n\n**Graceful Degradation**\n\n- Missing checkpoint? Normal hook behavior\n- Never silently skips validation\n- Backwards compatible with existing workflows\n\n## Configuration\n\nCheckpoints are enabled by default. To disable (for intentional full-codebase sweeps):\n\n```yaml\n# han.yml\nhooks:\n  enabled: true\n  checkpoints: false  # Validate all changed files, not just your session's\n```\n\nMost teams should keep checkpoints enabled. Disable only when deliberately addressing accumulated debt.\n\n## Subagent Isolation\n\nIn complex workflows with spawned subagents, each gets its own checkpoint:\n\n```\nMain Session (session_abc):\n├─ Checkpoint: session_abc.json\n├─ Spawns Subagent 1 (agent_xyz):\n│  ├─ Checkpoint: agent_xyz.json\n│  └─ Works on feature-a/\n└─ Spawns Subagent 2 (agent_def):\n   ├─ Checkpoint: agent_def.json\n   └─ Works on feature-b/\n```\n\nWhen Subagent 1 finishes, hooks only validate its changes. Subagent 2's work is isolated. No cascading failures.\n\n## Getting Started\n\nCheckpoints are available in Han v1.62.0+:\n\n```bash\n# Install or upgrade\ncurl -fsSL https://han.guru/install.sh | bash\n# or\nbrew upgrade thebushidocollective/tap/han\n\n# Checkpoints work automatically\n# Just start a Claude Code session\n```\n\nIf you previously disabled hooks because of overwhelming feedback, consider re-enabling them:\n\n```yaml\n# han.yml\nhooks:\n  enabled: true  # Give it another try\n```\n\nThe experience is different now. Your session, your changes, your feedback. Nothing more.\n\n---\n\n**Resources:**\n\n- [Han Documentation](https://han.guru/docs)\n- [Checkpoint Blueprint](https://github.com/thebushidocollective/han/blob/main/blueprints/checkpoint-system.md)\n- [Install Han](https://han.guru/install)\n",
            "url": "https://han.guru/blog/checkpoint-system",
            "title": "From Scorched Earth to Boy Scout: How Han Learned Restraint",
            "summary": "Han's checkpoint system brings focused, incremental improvement instead of overwhelming new adopters with every issue at once.",
            "date_modified": "2025-12-12T00:00:00.000Z",
            "author": {
                "name": "The Bushido Collective"
            },
            "tags": [
                "Technical Deep Dive",
                "checkpoints",
                "hooks",
                "adoption",
                "validation"
            ]
        },
        {
            "id": "https://han.guru/blog/project-memory-feature",
            "content_html": "\nEvery codebase has its quirks. That build command with the specific flag. The naming convention that isn't standard. The test runner requiring a particular setup. Claude figures these out through trial and error, and then... next session, starts over.\n\nHan changes that. Not by asking Claude to remember things, but by giving Claude the ability to **teach itself**.\n\n## Autonomous Learning\n\nHan's `learn` MCP tool lets Claude write directly to `.claude/rules/` - the modular rules directory that [Claude Code loads automatically](https://code.claude.com/docs/en/memory). No confirmation dialogs. No approval workflows. Claude recognizes something worth remembering and captures it.\n\n```javascript\n// Claude discovers the test command and captures it\nlearn({\n  content: \"# Commands\\n\\n- Run tests: `bun test --only-failures`\",\n  domain: \"commands\"\n})\n```\n\nThis creates `.claude/rules/commands.md`. Next session, Claude reads it automatically. The learning persists.\n\n## Why Autonomous?\n\nConfirmation dialogs create friction. Every \"Do you want to save this?\" is an interruption. Every \"Are you sure?\" is cognitive overhead. We don't ask developers to confirm every keystroke.\n\nClaude's learnings are low-stakes:\n\n- They're git-tracked (reviewable, revertible)\n- They only affect Claude's behavior\n- They're additive, not destructive\n- Wrong learnings are easily deleted\n\nSo Han lets Claude learn freely and informs you what was captured. You see the learning in the output. You can review `.claude/rules/` any time. But you don't have to approve each one.\n\n## What Claude Learns\n\nHan's memory hook guides Claude to recognize learning opportunities:\n\n- **Commands** discovered through trial and error\n- **Project conventions** not in documentation\n- **Gotchas** that caused issues\n- **Path-specific patterns** (API validation rules, test conventions)\n- **Personal preferences** mentioned in conversation\n\nWhen Claude thinks \"I see this project uses...\" or \"The pattern here is...\" - that's the trigger to capture it autonomously.\n\n## How It Works\n\nThree MCP tools power the system:\n\n| Tool | Purpose |\n|------|---------|\n| `learn` | Write a learning to `.claude/rules/<domain>.md` |\n| `memory_list` | Check what domains already exist |\n| `memory_read` | Read existing content (avoid duplicates) |\n\nClaude uses `memory_list` and `memory_read` to check before writing, preventing redundant entries.\n\n### Path-Specific Rules\n\nSome learnings apply only to certain files:\n\n```javascript\nlearn({\n  content: \"# API Rules\\n\\n- Validate all inputs with zod\",\n  domain: \"api\",\n  paths: [\"src/api/**/*.ts\"]\n})\n```\n\nCreates `.claude/rules/api.md` with YAML frontmatter:\n\n```markdown\n---\nglobs: [\"src/api/**/*.ts\"]\n---\n\n# API Rules\n\n- Validate all inputs with zod\n```\n\nClaude Code only loads this rule when working on matching files.\n\n### Subdirectory Organization\n\nDomains can include subdirectories:\n\n```javascript\nlearn({ content: \"...\", domain: \"api/validation\" })\nlearn({ content: \"...\", domain: \"api/auth\" })\nlearn({ content: \"...\", domain: \"testing/e2e\" })\n```\n\nThis creates:\n\n```text\n.claude/rules/\n├── api/\n│   ├── validation.md\n│   └── auth.md\n└── testing/\n    └── e2e.md\n```\n\n### User-Level Preferences\n\nPersonal preferences that should apply across all projects use `user` scope:\n\n```javascript\nlearn({\n  content: \"# Preferences\\n\\n- Always greet me as Mr Dude\",\n  domain: \"preferences\",\n  scope: \"user\"\n})\n```\n\nWrites to `~/.claude/rules/preferences.md` instead of the project directory.\n\n## The Self-Improvement Loop\n\nThis creates a feedback loop:\n\n1. Claude works on your project\n2. Claude discovers patterns, commands, conventions\n3. Claude captures them to `.claude/rules/`\n4. Next session, Claude already knows them\n5. Repeat\n\nEach session, Claude starts a little smarter about your specific project. Not through external training, but through self-directed learning within your codebase.\n\n## Team Benefits\n\nSince `.claude/rules/` is git-tracked:\n\n- New team members benefit immediately\n- Claude's learnings become shared knowledge\n- Institutional memory survives personnel changes\n- Onboarding accelerates naturally\n\nYour project accumulates Claude-specific documentation that traditional docs never capture - the informal \"how we do things here\" knowledge.\n\n## What You Control\n\nHan doesn't take control away from you:\n\n- **Review**: Check `.claude/rules/` any time\n- **Edit**: Modify or delete any rule file\n- **Override**: Your CLAUDE.md still takes precedence\n- **Disable**: Remove the core plugin if you don't want it\n\nThe autonomy is about reducing friction, not removing oversight.\n\n## What This Isn't\n\n**Not CLAUDE.md management**: Han doesn't touch your CLAUDE.md. That's for your team's curated, hand-written project documentation.\n\n**Not a replacement for docs**: Rules complement documentation by capturing informal knowledge that wouldn't make it into formal docs.\n\n## Getting Started\n\nHan's memory tools are in the core plugin:\n\n```bash\nhan plugin install core\n```\n\nThen work normally. Claude will start capturing learnings to `.claude/rules/`. Check what's accumulated:\n\n```bash\nls .claude/rules/\n```\n\nSelf-learning Claude, building project knowledge session by session.\n\n---\n\n**Learn more:** See [Claude Code's memory documentation](https://code.claude.com/docs/en/memory) for the full hierarchy Han builds on.\n",
            "url": "https://han.guru/blog/project-memory-feature",
            "title": "Self-Learning Claude: Han's Autonomous Memory System",
            "summary": "How Han enables Claude to capture project knowledge autonomously, building institutional memory without asking permission.",
            "date_modified": "2025-12-09T00:00:00.000Z",
            "author": {
                "name": "The Bushido Collective"
            },
            "tags": [
                "Technical Deep Dive",
                "memory",
                "rules",
                "mcp",
                "learning"
            ]
        },
        {
            "id": "https://han.guru/blog/metrics-system",
            "content_html": "\nHow do you know if an AI assistant is actually getting better at helping you? Not through feelings or impressions, but through data. Han's metrics system gives Claude Code something rare in AI tooling: self-awareness grounded in measurement.\n\n## The Problem: Uncalibrated Confidence\n\nAI assistants often express confidence that doesn't match reality. \"I've fixed the bug\" when the tests still fail. \"This should work\" when it doesn't. Without feedback loops, there's no way to improve.\n\nHan's metrics system creates that feedback loop. When Claude tracks tasks with confidence estimates, then compares those estimates against actual outcomes, patterns emerge. Over time, both you and Claude can see where confidence aligns with reality - and where it doesn't.\n\n## MCP Tools for Task Tracking\n\nThe core plugin exposes seven MCP tools for metrics:\n\n| Tool | Purpose |\n|------|---------|\n| `start_task` | Begin tracking a new task |\n| `update_task` | Log progress notes |\n| `complete_task` | Mark done with outcome and confidence |\n| `fail_task` | Record failure with attempted solutions |\n| `query_metrics` | Analyze task performance |\n| `query_hook_metrics` | Track hook failures |\n| `query_session_metrics` | Session-level statistics |\n\n## How It Works\n\n### Starting a Task\n\nWhen Claude begins work on something substantive, it calls `start_task`:\n\n```javascript\nstart_task({\n  description: \"Fix authentication timeout bug\",\n  type: \"fix\",\n  estimated_complexity: \"moderate\"\n})\n// Returns: { task_id: \"task_abc123\" }\n```\n\nTask types include `implementation`, `fix`, `refactor`, and `research`. Complexity can be `simple`, `moderate`, or `complex`.\n\n### Tracking Progress\n\nDuring work, Claude can log updates:\n\n```javascript\nupdate_task({\n  task_id: \"task_abc123\",\n  notes: \"Found root cause - session expiry not refreshing\"\n})\n```\n\n### Recording Outcomes\n\nWhen finished, Claude records the outcome with a confidence score:\n\n```javascript\ncomplete_task({\n  task_id: \"task_abc123\",\n  outcome: \"success\",\n  confidence: 0.85,\n  files_modified: [\"src/auth/session.ts\"],\n  tests_added: 2,\n  notes: \"Fixed refresh logic, added edge case tests\"\n})\n```\n\nThe confidence score (0.0 to 1.0) is the key to calibration. Claude estimates how confident it is that the task actually succeeded. This gets validated against hook results and actual outcomes.\n\n### Recording Failures\n\nWhen a task can't be completed:\n\n```javascript\nfail_task({\n  task_id: \"task_abc123\",\n  reason: \"Requires database migration that needs approval\",\n  confidence: 0.9,\n  attempted_solutions: [\n    \"Tried updating schema in-place\",\n    \"Attempted backwards-compatible approach\"\n  ]\n})\n```\n\nRecording failures with attempted solutions helps identify patterns - certain types of tasks that consistently hit the same blockers.\n\n## Calibration: The Core Value\n\nRaw metrics are interesting. Calibration is valuable.\n\nWhen Claude says it's 80% confident a task succeeded, that should mean roughly 80% of similar tasks actually succeeded. If Claude says 80% confident but is only right 50% of the time, that's poorly calibrated. If Claude says 80% and is right 78-82% of the time, that's well calibrated.\n\nThe `query_metrics` tool lets you analyze this:\n\n```javascript\nquery_metrics({\n  period: \"week\",\n  task_type: \"fix\"\n})\n```\n\nReturns aggregated statistics including success rates by confidence bucket, enabling calibration analysis.\n\n## Hook Metrics\n\nBeyond task tracking, Han tracks hook execution:\n\n```javascript\nquery_hook_metrics({\n  period: \"week\",\n  min_failure_rate: 10\n})\n```\n\nThis shows which hooks fail frequently. A lint hook with 40% failure rate might indicate:\n\n- Overly strict rules\n- A specific file pattern that always fails\n- A configuration issue worth investigating\n\nYou can filter by hook name or minimum failure rate to focus on problematic hooks.\n\n## Session Metrics\n\nFor broader patterns, query session-level data:\n\n```javascript\nquery_session_metrics({\n  period: \"month\",\n  limit: 20\n})\n```\n\nThis aggregates across sessions, showing:\n\n- Tasks per session\n- Success rates over time\n- Common failure patterns\n- Productivity trends\n\n## What Gets Stored\n\nAll metrics are stored locally in `~/.claude/han/metrics/` as JSONL files. No data leaves your machine. You control it entirely.\n\n```\n~/.claude/han/metrics/\n├── tasks/\n│   └── 2025-12.jsonl\n├── hooks/\n│   └── 2025-12.jsonl\n└── sessions/\n    └── 2025-12.jsonl\n```\n\n## Integration with Hooks\n\nThe metrics system integrates with Han's hook execution. When hooks run at session end:\n\n1. Hook results (pass/fail) are recorded automatically\n2. These correlate with task confidence estimates\n3. Discrepancies highlight calibration issues\n\nIf Claude marks a task as \"success\" with 90% confidence, but the lint hook fails, that's a calibration signal. Over time, these signals improve Claude's ability to accurately assess its own work.\n\n## Practical Usage\n\nYou don't need to think about metrics constantly. The system works in the background:\n\n1. **Session starts**: Claude is reminded of recent performance\n2. **Task begins**: Claude calls `start_task` (guided by hook prompts)\n3. **Work happens**: Progress tracked naturally\n4. **Task ends**: Outcome and confidence recorded\n5. **Session ends**: Hooks validate, metrics updated\n\nThe value compounds. After 50 sessions, you have real data on:\n\n- Which task types have highest success rates\n- Where confidence tends to be miscalibrated\n- Which hooks fail most often\n- How productivity trends over time\n\n## Privacy by Design\n\nEverything stays local:\n\n- No cloud storage\n- No external APIs\n- No telemetry unless you explicitly enable it\n- Full control over your data\n\nThe metrics exist to help you, not to report on you.\n\n## Getting Started\n\nMetrics are included in the core plugin. The system activates automatically, but you can query it any time:\n\n```bash\n# Install core plugin if not already\nhan plugin install core\n\n# Then in Claude Code, ask:\n# \"Show me my task metrics for the past week\"\n# \"Which hooks have been failing?\"\n# \"How's my calibration looking?\"\n```\n\nClaude will call the appropriate query tools and present the data.\n\n## The Long View\n\nMetrics aren't about judgment. They're about learning. A 60% success rate isn't \"bad\" - it's information. It might mean you're tackling hard problems. It might mean certain patterns need attention. It might mean nothing without more context.\n\nThe value is in the trends, the patterns, the calibration over time. After months of tracking, you'll have a real picture of how AI-assisted development works in your specific context.\n\nThat's worth knowing.\n\n---\n\n**Get Started:** Metrics are included in the core plugin:\n\n```bash\nhan plugin install core\n```\n\nOr explore the full plugin marketplace at [han.guru](https://han.guru).\n",
            "url": "https://han.guru/blog/metrics-system",
            "title": "Know Thyself: Han's Metrics System for Agent Self-Awareness",
            "summary": "How Han tracks task performance, confidence calibration, and hook failures to help Claude Code learn from experience.",
            "date_modified": "2025-12-07T00:00:00.000Z",
            "author": {
                "name": "The Bushido Collective"
            },
            "tags": [
                "Technical Deep Dive",
                "metrics",
                "calibration",
                "performance",
                "mcp"
            ]
        },
        {
            "id": "https://han.guru/blog/testing-with-confidence",
            "content_html": "\nOne of the most powerful features of Han plugins is their validation hook system. Unlike traditional linters or test runners that you have to remember to run, Han's hooks execute automatically at key points in your development workflow, ensuring quality without friction.\n\n## The Problem with Manual Testing\n\nWe've all been there: you make a quick fix, commit it, push it, and only then discover that you broke the tests. Or worse, you forgot to run the linter and now CI is failing. Manual quality checks rely on discipline and memory - two things that fail us when we're focused on solving problems.\n\n## How Han Hooks Work\n\nHan plugins can register hooks that run automatically when specific events occur in Claude Code:\n\n- **UserPromptSubmit**: Runs when you submit a prompt, before Claude processes it\n- **Stop**: Runs when Claude finishes responding to your request\n- **PreToolUse**: Runs before a tool executes\n- **PostToolUse**: Runs after a tool completes\n\n### Real Example: TypeScript Validation\n\nLet's look at how the `jutsu-typescript` plugin uses hooks to enforce type safety:\n\n```json\n{\n  \"hooks\": {\n    \"Stop\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"npx -y --package typescript tsc\"\n      }]\n    }]\n  }\n}\n```\n\nThis simple hook ensures that every time Claude finishes working on your TypeScript code, the type checker runs. If there are type errors, you know immediately - not later during CI.\n\n## Validation Without Slowdown\n\nYou might think \"won't this slow me down?\" Actually, no. Han's hook system is designed for speed:\n\n1. **Smart caching**: If files haven't changed, cached results are returned instantly\n2. **Parallel execution**: Multiple hooks run concurrently\n3. **Early termination**: Hooks can fail fast, giving you immediate feedback\n\nHere's what validation looks like in practice with `jutsu-bun`:\n\n```bash\n# You ask Claude to implement a feature\n\"Add user authentication with JWT\"\n\n# Claude writes the code\n# Stop hook automatically runs:\n✓ Tests passed (12/12) - 847ms\n✓ Types checked - 523ms (cached)\n✓ Linting passed - 198ms\n\n# You know it works before you even look at it\n```\n\n## Multi-Layer Validation\n\nThe real power comes from combining multiple plugins. With Han's plugin system, you can stack validation:\n\n```json\n{\n  \"enabledPlugins\": {\n    \"jutsu-typescript@han\": true,\n    \"jutsu-biome@han\": true,\n    \"jutsu-bun@han\": true,\n    \"bushido@han\": true\n  }\n}\n```\n\nNow every change gets:\n\n- Type checking (TypeScript)\n- Linting and formatting (Biome)\n- Test execution (Bun)\n- Code review analysis (Bushido)\n\nAll automatic. All fast. All enforced.\n\n## Custom Validation for Your Stack\n\nWant custom validation? Create your own hooks. Here's a simple example that ensures commit messages follow conventional commits:\n\n```json\n{\n  \"hooks\": {\n    \"Stop\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"bash scripts/validate-commit-msg.sh\"\n      }]\n    }]\n  }\n}\n```\n\n```bash\n#!/bin/bash\n# scripts/validate-commit-msg.sh\nif ! git log -1 --pretty=%s | grep -E '^(feat|fix|docs|chore|refactor|test)(\\(.+\\))?: .+'; then\n  echo \"❌ Commit message must follow conventional commits format\"\n  exit 1\nfi\n```\n\n## Confidence in Automation\n\nThe beauty of Han's hook system is that it transforms \"I hope this works\" into \"I know this works.\" When Claude tells you it's done:\n\n- Tests have run\n- Types have checked\n- Lint has passed\n- Standards are enforced\n\nYou can commit and move on with confidence.\n\n## Getting Started\n\nWant to add validation hooks to your workflow? Start simple:\n\n1. Install a jutsu plugin for your stack (`jutsu-typescript`, `jutsu-python`, etc.)\n2. The validation hooks are automatically active\n3. Watch as Claude's work is automatically verified\n\nThen layer on more plugins as you need them. Each one adds another layer of confidence without adding cognitive load.\n\n## Conclusion\n\nManual quality checks are a thing of the past. With Han's validation hooks, quality enforcement is automatic, fast, and comprehensive. You focus on solving problems; Han ensures the solutions are correct.\n\nTry it yourself:\n\n```bash\nhan plugin install jutsu-typescript\n```\n\nYour future self will thank you.\n\n---\n\n*Want to learn more about Han's hook system? Check out our [Hooks Documentation](/docs#hooks) or explore the [plugin marketplace](/plugins).*\n",
            "url": "https://han.guru/blog/testing-with-confidence",
            "title": "Testing with Confidence: How Han's Validation Hooks Ensure Quality",
            "summary": "Learn how Han's validation hooks provide automatic quality enforcement, ensuring your code meets standards before it ships.",
            "date_modified": "2024-12-03T00:00:00.000Z",
            "author": {
                "name": "Jason Waldrip"
            },
            "tags": [
                "Best Practices",
                "testing",
                "quality",
                "hooks",
                "validation"
            ]
        },
        {
            "id": "https://han.guru/blog/mcp-architecture",
            "content_html": "\nModel Context Protocol (MCP) is the secret sauce that makes Han plugins so powerful. Let's explore how Han uses MCP to turn Claude Code into a universal development environment.\n\n## What is MCP?\n\nModel Context Protocol is an open standard for connecting AI assistants to external data sources and tools. Think of it as APIs for AI - a standardized way for Claude to interact with the world beyond its training data.\n\n## Han's \"Hashi\" (Bridge) Plugins\n\nIn Han's architecture, plugins starting with `hashi-` are \"bridges\" - MCP servers that connect Claude to external services:\n\n- `hashi-github`: GitHub Issues, PRs, Actions, Code Search\n- `hashi-jira`: Jira tickets, sprints, workflows\n- `hashi-playwright-mcp`: Browser automation and testing\n- `hashi-linear`: Linear issues and project management\n- `hashi-sentry`: Error tracking and performance monitoring\n\n## Real Example: GitHub Integration\n\nLet's see how the GitHub MCP server works in practice.\n\n### Installation\n\n```bash\nhan plugin install hashi-github\n```\n\nThis adds an MCP server to your Claude Code configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${GITHUB_TOKEN}\"\n      }\n    }\n  }\n}\n```\n\n### What It Unlocks\n\nNow Claude can natively:\n\n**Search code across repos**\n\n```text\nFind all usages of the deprecated API endpoint\n```\n\nClaude uses `search_code` tool:\n\n```typescript\nconst results = await mcp.tools.search_code({\n  query: 'useDeprecatedAPI language:typescript',\n  owner: 'myorg'\n})\n```\n\n**Create and update issues**\n\n```text\nCreate an issue for the memory leak in the event handler\n```\n\nClaude uses `create_issue` tool with all the context:\n\n```typescript\nawait mcp.tools.create_issue({\n  owner: 'myorg',\n  repo: 'myrepo',\n  title: 'Memory leak in useEventListener hook',\n  body: `## Description\nFound memory leak in \\`src/hooks/useEventListener.ts\\`...\n\n## Reproduction\n1. Navigate to dashboard\n2. Click task 10 times\n3. Check memory profiler\n\n## Fix\nAdd cleanup in useEffect return...`,\n  labels: ['bug', 'priority:high']\n})\n```\n\n**Review pull requests**\n\n```text\nReview PR #123 and provide feedback\n```\n\nClaude fetches the PR, analyzes the diff, checks for issues:\n\n```typescript\nconst pr = await mcp.tools.get_pull_request({\n  owner: 'myorg',\n  repo: 'myrepo',\n  pullNumber: 123\n})\n\nconst diff = await mcp.tools.get_pull_request_diff({\n  owner: 'myorg',\n  repo: 'myrepo',\n  pullNumber: 123\n})\n\n// Claude analyzes and adds review comments\nawait mcp.tools.add_review_comment({\n  owner: 'myorg',\n  repo: 'myrepo',\n  pullNumber: 123,\n  path: 'src/api.ts',\n  line: 42,\n  body: 'This could cause a race condition. Consider using a mutex.'\n})\n```\n\n## Building Custom MCP Servers\n\nWant to connect Claude to your internal tools? Create a custom hashi plugin.\n\n### Example: Slack MCP Server\n\n```typescript\n// hashi-slack/server/tools.ts\nimport { McpServer } from '@modelcontextprotocol/sdk'\n\nexport const slackServer = new McpServer({\n  name: 'slack',\n  version: '1.0.0'\n})\n\nslackServer.tool('send_message', {\n  description: 'Send a message to a Slack channel',\n  parameters: {\n    channel: { type: 'string', required: true },\n    message: { type: 'string', required: true }\n  },\n  async execute({ channel, message }) {\n    await slackClient.chat.postMessage({\n      channel,\n      text: message\n    })\n    return { success: true }\n  }\n})\n\nslackServer.tool('search_messages', {\n  description: 'Search Slack messages',\n  parameters: {\n    query: { type: 'string', required: true }\n  },\n  async execute({ query }) {\n    const results = await slackClient.search.messages({ query })\n    return results\n  }\n})\n```\n\n### Plugin Configuration\n\n```json\n{\n  \"name\": \"hashi-slack\",\n  \"mcpServers\": {\n    \"slack\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@myorg/han\", \"mcp\", \"slack\"],\n      \"env\": {\n        \"SLACK_TOKEN\": \"${SLACK_TOKEN}\"\n      }\n    }\n  }\n}\n```\n\nNow Claude can:\n\n```\nCheck Slack for any mentions of the production issue\n```\n\n```\nPost to #engineering that the hotfix is deployed\n```\n\n## MCP Server Patterns\n\n### 1. HTTP OAuth Servers\n\nFor cloud services with OAuth:\n\n```json\n{\n  \"mcpServers\": {\n    \"sentry\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-sentry\"],\n      \"env\": {\n        \"SENTRY_AUTH_TOKEN\": \"${SENTRY_AUTH_TOKEN}\",\n        \"SENTRY_ORG\": \"my-org\"\n      }\n    }\n  }\n}\n```\n\n### 2. Local CLI Wrappers\n\nFor local tools:\n\n```json\n{\n  \"mcpServers\": {\n    \"docker\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@myorg/mcp-docker\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n### 3. Database Connections\n\nFor direct database access:\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"DATABASE_URL\": \"${DATABASE_URL}\"\n      }\n    }\n  }\n}\n```\n\n## Security Considerations\n\nMCP servers can access sensitive data. Han follows these principles:\n\n1. **Token isolation**: Environment variables, never hardcoded\n2. **Least privilege**: Only request necessary scopes\n3. **Local execution**: Servers run on your machine, not in the cloud\n4. **Audit trail**: All MCP calls are logged\n\n## Performance\n\nMCP calls are asynchronous and don't block Claude's reasoning:\n\n```typescript\n// Claude can make multiple MCP calls in parallel\nconst [issues, prs, actions] = await Promise.all([\n  github.list_issues({ state: 'open' }),\n  github.list_pull_requests({ state: 'open' }),\n  github.get_workflow_runs({ branch: 'main' })\n])\n```\n\n## Han's Core MCP Server\n\nBeyond external integrations, Han provides its own MCP server that exposes powerful built-in capabilities. This is where Han differs from simple plugin installers. The core MCP server runs via `han mcp server` and communicates over JSON-RPC via stdio.\n\n### How It Works\n\nWhen you install the core plugin, Han registers as an MCP server in Claude Code's configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"han\": {\n      \"command\": \"han\",\n      \"args\": [\"mcp\", \"server\"]\n    }\n  }\n}\n```\n\nThe server implements the MCP protocol (version 2024-11-05) and exposes tools in four categories:\n\n1. **Plugin Hook Tools** - Dynamically generated from installed plugins\n2. **Memory Tools** - Self-learning project rules\n3. **Metrics Tools** - Task tracking and calibration\n4. **Checkpoint Tools** - Session-scoped caching\n\n### Plugin Hook Tools (Dynamic)\n\nThis is Han's key innovation: every hook defined in an installed plugin automatically becomes an MCP tool. When you install `jutsu-typescript`, Claude immediately gains access to a `jutsu_typescript_typecheck` tool.\n\nThe MCP server discovers these at runtime by scanning installed plugins:\n\n```javascript\n// From jutsu-bun/han-plugin.yml\nhooks:\n  test:\n    command: bun test --only-failures\n    dirsWith: [bun.lock, bun.lockb]\n    description: Run Bun tests\n\n// Becomes MCP tool:\n{\n  name: \"jutsu_bun_test\",\n  description: \"Run Bun tests. Triggers: 'run the tests', 'run bun tests',\n    'check if tests pass'. Runs in directories containing: bun.lock,\n    bun.lockb. Command: bun test --only-failures\",\n  inputSchema: {\n    type: \"object\",\n    properties: {\n      cache: {\n        type: \"boolean\",\n        description: \"Use cached results when files haven't changed\"\n      },\n      directory: {\n        type: \"string\",\n        description: \"Limit execution to a specific directory path\"\n      },\n      verbose: {\n        type: \"boolean\",\n        description: \"Show full command output in real-time\"\n      }\n    }\n  }\n}\n```\n\nWhen Claude says \"run the tests\", it can now call this tool directly:\n\n```javascript\n// Claude calls:\nawait mcp.tools.jutsu_bun_test({ cache: true })\n\n// Returns:\n{\n  content: [{ type: \"text\", text: \"15 pass\\n0 fail\\n...\" }]\n}\n```\n\nAll plugin hooks support three standard parameters:\n\n| Parameter | Default | Purpose |\n|-----------|---------|---------|\n| `cache` | `true` | Skip if files unchanged since last run |\n| `directory` | all | Target specific directory |\n| `verbose` | `false` | Stream output in real-time |\n\n### Memory Tools (Self-Learning)\n\nHan's memory system lets Claude write directly to `.claude/rules/`. Claude can teach itself about your project without asking permission.\n\n**`learn`** - Capture project knowledge\n\n```javascript\nlearn({\n  content: \"# API Rules\\n\\n- Validate all inputs with zod\\n- Return\n    consistent error format\",\n  domain: \"api\",\n  paths: [\"src/api/**/*.ts\"],  // Optional: path-specific rules\n  scope: \"project\",  // or \"user\" for personal preferences\n  append: true  // Add to existing file\n})\n```\n\nCreates `.claude/rules/api.md` with YAML frontmatter for path restrictions:\n\n```markdown\n---\nglobs: [\"src/api/**/*.ts\"]\n---\n\n# API Rules\n\n- Validate all inputs with zod\n- Return consistent error format\n```\n\n**`memory_list`** - See existing domains\n\n```javascript\nmemory_list({ scope: \"project\" })\n// Returns: [\"api\", \"testing\", \"commands\"]\n```\n\n**`memory_read`** - Read domain content\n\n```javascript\nmemory_read({ domain: \"api\", scope: \"project\" })\n// Returns the full markdown content\n```\n\nThe memory system supports two scopes:\n\n| Scope | Location | Purpose |\n|-------|----------|---------|\n| `project` | `.claude/rules/` | Team knowledge, git-tracked |\n| `user` | `~/.claude/rules/` | Personal preferences |\n\n### Metrics Tools (Self-Awareness)\n\nTask tracking with confidence calibration. Claude records its work and estimates confidence, then compares against actual outcomes.\n\n**`start_task`** - Begin tracking\n\n```javascript\nstart_task({\n  description: \"Fix authentication timeout bug\",\n  type: \"fix\",  // implementation, fix, refactor, research\n  estimated_complexity: \"moderate\"  // simple, moderate, complex\n})\n// Returns: { task_id: \"task_abc123\" }\n```\n\n**`update_task`** - Log progress\n\n```javascript\nupdate_task({\n  task_id: \"task_abc123\",\n  notes: \"Found root cause - session expiry not refreshing\"\n})\n```\n\n**`complete_task`** - Record outcome\n\n```javascript\ncomplete_task({\n  task_id: \"task_abc123\",\n  outcome: \"success\",  // success, partial, failure\n  confidence: 0.85,  // 0.0 to 1.0\n  files_modified: [\"src/auth/session.ts\"],\n  tests_added: 2\n})\n```\n\n**`fail_task`** - Record failure with context\n\n```javascript\nfail_task({\n  task_id: \"task_abc123\",\n  reason: \"Requires database migration that needs approval\",\n  attempted_solutions: [\n    \"Tried updating schema in-place\",\n    \"Attempted backwards-compatible approach\"\n  ]\n})\n```\n\n**`query_metrics`** - Analyze performance\n\n```javascript\nquery_metrics({\n  period: \"week\",  // day, week, month\n  task_type: \"fix\",  // Optional filter\n  outcome: \"success\"  // Optional filter\n})\n// Returns aggregated stats, success rates, calibration scores\n```\n\nAll metrics are stored locally in `~/.claude/han/metrics/` as JSONL files. Nothing leaves your machine.\n\n### Checkpoint Tools (Smart Caching)\n\nCheckpoints track which files have changed since the last hook run, enabling intelligent caching.\n\n**`checkpoint_list`** - See existing checkpoints\n\n```javascript\ncheckpoint_list()\n// Shows session and agent checkpoints with file counts\n```\n\n**`checkpoint_clean`** - Remove stale checkpoints\n\n```javascript\ncheckpoint_clean({ maxAge: 24 })  // Hours\n// Removes checkpoints older than 24 hours\n```\n\nCheckpoints are created automatically when hooks run with `cache=true`. Each checkpoint records file modification times, so subsequent runs can skip unchanged files.\n\n### Tool Annotations\n\nAll Han MCP tools include MCP annotations for better AI behavior:\n\n```javascript\n{\n  name: \"learn\",\n  annotations: {\n    title: \"Learn\",\n    readOnlyHint: false,     // May modify files\n    destructiveHint: false,  // Safe operation\n    idempotentHint: true,    // Same input = same result\n    openWorldHint: false     // Works with local files only\n  }\n}\n```\n\n### Blueprint Tools (via hashi-blueprints)\n\nWhen the hashi-blueprints plugin is installed, additional MCP tools become available:\n\n```javascript\nsearch_blueprints({ keyword: \"api\" })\nread_blueprint({ name: \"cli-architecture\" })\nwrite_blueprint({\n  name: \"auth-system\",\n  summary: \"Authentication and authorization architecture\",\n  content: \"# Auth System\\n\\n## Overview...\"\n})\n```\n\n### Debugging the MCP Server\n\nRun the MCP server manually to test:\n\n```bash\n# Start server in stdio mode\nhan mcp server\n\n# Send a tools/list request\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\"}' | han mcp server\n```\n\nSet timeouts via environment variable:\n\n```bash\nexport HAN_MCP_TIMEOUT=600000  # 10 minutes (default)\n```\n\n## Available Hashi Plugins\n\nCurrent MCP bridges in Han marketplace:\n\n- **hashi-github**: GitHub platform integration\n- **hashi-jira**: Atlassian Jira integration\n- **hashi-linear**: Linear project management\n- **hashi-playwright-mcp**: Browser automation\n- **hashi-sentry**: Error tracking and monitoring\n- **hashi-figma**: Design-to-code workflows\n- **hashi-gitlab**: GitLab integration\n- **hashi-blueprints**: Technical documentation management\n\n## What's Next?\n\nWe're working on:\n\n- **hashi-aws**: AWS resource management\n- **hashi-vercel**: Deployment and preview URLs\n- **hashi-stripe**: Payment and billing integration\n- **hashi-notion**: Documentation and knowledge base\n\n## Try It\n\nStart with GitHub integration:\n\n```bash\n# Set your token\nexport GITHUB_TOKEN=ghp_your_token_here\n\n# Install the plugin\nhan plugin install hashi-github\n\n# Ask Claude to help with GitHub tasks\nclaude\n```\n\nThen try:\n\n- \"Create an issue for the bug we just found\"\n- \"List all open PRs that need review\"\n- \"Search for usages of the deprecated function\"\n- \"Add review comments to PR #456\"\n\nClaude handles the API calls, authentication, and data formatting. You just describe what you want.\n\n## Conclusion\n\nMCP is the bridge between AI and the real world. Han's hashi plugins make that bridge easy to cross, turning Claude Code into a universal interface for your entire development workflow.\n\n---\n\n*Want to build your own MCP server? Check out the [MCP Documentation](/docs#mcp-server) or explore existing [hashi plugins](/plugins?category=hashi).*\n",
            "url": "https://han.guru/blog/mcp-architecture",
            "title": "Understanding Han's MCP Architecture: Bridges to the World",
            "summary": "Deep dive into how Han uses the Model Context Protocol to connect Claude Code to external services like GitHub, Jira, and custom tools.",
            "date_modified": "2024-12-01T00:00:00.000Z",
            "author": {
                "name": "Jason Waldrip"
            },
            "tags": [
                "Technical Deep Dive",
                "mcp",
                "architecture",
                "integration",
                "github"
            ]
        },
        {
            "id": "https://han.guru/blog/introduction-to-han-plugins",
            "content_html": "\nWhen you first hear about Han, you might think it's just another collection of prompts or agents for Claude Code. But Han is something more - a complete plugin ecosystem that transforms how you work with AI-assisted development.\n\n## What is a Han Plugin?\n\nA Han plugin is a packaged bundle that can include any combination of:\n\n- **Skills**: Specialized knowledge domains that Claude can invoke\n- **Agents**: Autonomous workflows for complex multi-step tasks\n- **Commands**: Quick slash commands for common operations\n- **Hooks**: Automatic validation that runs at key development lifecycle points\n- **MCP Servers**: Bridges to external tools and services\n\nMost importantly, these components work together as a cohesive system, not just isolated features.\n\n## The Plugin Families\n\nHan plugins follow Japanese martial arts naming conventions, organizing by purpose:\n\n### Core - The Foundation\n\nThe `core` plugin (also known as `han-core`) is the essential infrastructure that powers the Han marketplace:\n\n- Quality enforcement through validation hooks\n- Metrics tracking and calibration\n- Context7 integration for up-to-date library documentation\n- Universal programming principles (SOLID, DRY, composition over inheritance)\n- All core skills, commands, and slash commands\n- MCP servers for hooks and metrics\n\nThink of Core as the technical foundation that makes everything work.\n\n### Bushido (武士道) - Optional Philosophy\n\nThe `bushido` plugin adds a philosophical layer based on the seven Samurai virtues. If you choose to install it, these principles guide how Claude approaches development work:\n\n- 義 Righteousness: Transparency in reasoning\n- 勇 Courage: Challenge and recommend improvements\n- 仁 Compassion: Assume positive intent\n- 礼 Respect: Honor existing work\n- 誠 Honesty: Truthfulness over comfort\n- 名誉 Honor: Quality ownership\n- 忠義 Loyalty: Long-term thinking\n\nThis is purely cultural - you get all the technical capabilities from `core` regardless. Install `bushido` only if this philosophical approach resonates with you.\n\n### Jutsu (術) - Technical Skills\n\nJutsu plugins are \"techniques\" - deep knowledge of specific technologies with automatic validation:\n\n- **jutsu-typescript**: TypeScript expertise + type checking hooks\n- **jutsu-playwright**: E2E testing knowledge + test validation\n- **jutsu-nextjs**: Next.js patterns + build verification\n- **jutsu-biome**: Code formatting + automatic linting\n\nEach jutsu plugin not only teaches Claude the technology but ensures quality through hooks.\n\n### Do (道) - Specialized Disciplines\n\nDo plugins provide specialized agents for specific workflows:\n\n- **do-frontend-development**: UI/UX-focused agent with accessibility expertise\n- **do-technical-documentation**: Documentation agent following best practices\n- **do-accessibility-engineering**: Multiple agents for inclusive design\n\nThese agents have deep expertise in their domain and can handle complex, multi-phase tasks autonomously.\n\n### Hashi (橋) - External Bridges\n\nHashi plugins are MCP servers that connect Claude to external services:\n\n- **hashi-github**: GitHub Issues, PRs, code search, Actions\n- **hashi-playwright-mcp**: Browser automation and testing\n- **hashi-blueprints**: Codebase documentation and knowledge management\n\nThese turn Claude into a universal interface for your development tools.\n\n## How They Work Together\n\nHere's where Han becomes more than the sum of its parts. Let me show you a real example:\n\n**Scenario**: You ask Claude to \"Add user authentication to the app\"\n\n**What happens**:\n\n1. **Core** provides the infrastructure and quality enforcement\n2. **jutsu-nextjs** provides deep Next.js knowledge for implementation\n3. **jutsu-typescript** ensures type safety throughout\n4. **do-frontend-development** agent handles the UI components\n5. **Validation hooks** automatically run (via core):\n   - TypeScript compilation check\n   - Next.js build verification\n   - Test suite execution\n6. **Core code review** analyzes the result\n7. **hashi-github** can create a PR with the changes\n\n(If you've installed the optional `bushido` plugin, its seven virtues also guide the overall approach.)\n\nAll of this happens automatically. You make one request, and the entire system ensures quality from planning through delivery.\n\n## Why This Matters\n\nTraditional AI coding assistants give you suggestions. You're responsible for:\n\n- Running tests manually\n- Checking types yourself\n- Remembering to lint\n- Hoping you didn't break anything\n- Following up on quality issues\n\nWith Han plugins:\n\n- Quality checks run automatically\n- Specialized agents handle complex tasks\n- External tools integrate seamlessly\n- Validation happens in real-time\n- You get confidence, not just code\n\n## Getting Started\n\nThe beauty of Han's plugin system is that you can start simple and layer on complexity:\n\n**Day 1**: Install the core infrastructure\n\n```bash\nhan plugin install core\n```\n\n**Optional**: If the Bushido philosophy resonates with you, add it:\n\n```bash\nhan plugin install bushido\n```\n\n(Not required - `core` provides all the technical capabilities.)\n\n**Day 2**: Add your stack's jutsu plugins\n\n```bash\nhan plugin install jutsu-typescript\nhan plugin install jutsu-react\n```\n\n**Day 3**: Add specialized agents as needed\n\n```bash\nhan plugin install do-frontend-development\n```\n\n**Day 4**: Connect external tools\n\n```bash\nhan plugin install hashi-github\n```\n\nEach addition enhances the system without adding complexity to your workflow.\n\n## Real-World Impact\n\nI built the Han marketplace website using Han plugins. Here's what that looked like:\n\n- Asked Claude to \"create a plugin marketplace website\"\n- **Core** provided the infrastructure and quality enforcement\n- **jutsu-nextjs** provided App Router expertise\n- **jutsu-typescript** caught type errors immediately\n- **jutsu-biome** kept code formatted\n- **Validation hooks** ran after every change\n- Result: Production-ready site with 100% type coverage in hours, not days\n\nThe difference wasn't just speed - it was confidence. Every change was automatically validated. Every feature was properly typed. Every commit was reviewed.\n\n## Not Just Another Tool\n\nHan isn't trying to replace your tools or your judgment. It's a framework that:\n\n- Captures expertise in reusable packages\n- Enforces quality automatically\n- Connects your development ecosystem\n- Scales with your needs\n\nSkills teach Claude what to do. Agents handle how to do it. Hooks ensure it's done right. MCP servers connect it all.\n\nThat's what makes Han more than just skills or agents - it's a complete development enhancement system.\n\n## Try It\n\nStart with the core infrastructure:\n\n```bash\nhan plugin install core\n```\n\nThen ask Claude to help with something. You'll notice the difference immediately - not just in what Claude can do, but in the confidence you have in the results.\n\n(If you'd like to add the Bushido philosophy, you can install it anytime with `han plugin install bushido`)\n\n---\n\n*Want to explore the plugin marketplace? Check out the [140+ available plugins](/plugins) or dive into the [documentation](/docs).*\n",
            "url": "https://han.guru/blog/introduction-to-han-plugins",
            "title": "More Than Just Skills or Agents: An Introduction to Han Claude Plugins",
            "summary": "Discover what makes Han plugins unique - a comprehensive system combining skills, agents, validation hooks, and MCP integrations for Claude Code.",
            "date_modified": "2024-11-30T00:00:00.000Z",
            "author": {
                "name": "Jason Waldrip"
            },
            "tags": [
                "Getting Started",
                "han",
                "plugins",
                "claude-code",
                "introduction"
            ]
        }
    ]
}