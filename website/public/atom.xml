<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://han.guru</id>
    <title>Han Blog - Claude Code Plugin Marketplace</title>
    <updated>2026-01-21T22:32:14.900Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <author>
        <name>The Bushido Collective</name>
        <email>info@han.guru</email>
        <uri>https://han.guru</uri>
    </author>
    <link rel="alternate" href="https://han.guru"/>
    <link rel="self" href="https://han.guru/atom.xml"/>
    <subtitle>News, tutorials, and insights about Han plugins for Claude Code built on Bushido principles</subtitle>
    <logo>https://han.guru/og-image.png</logo>
    <icon>https://han.guru/favicon.ico</icon>
    <rights>© 2026 The Bushido Collective</rights>
    <entry>
        <title type="html"><![CDATA[Introducing AI-DLC 2026: A Methodology for Autonomous Development]]></title>
        <id>https://han.guru/blog/ai-dlc-2026-paper</id>
        <link href="https://han.guru/blog/ai-dlc-2026-paper"/>
        <updated>2026-01-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We're publishing a comprehensive methodology for AI-driven software development, introducing human-on-the-loop workflows, backpressure-driven quality, and autonomous development loops.]]></summary>
        <content type="html"><![CDATA[
Software development has entered a new era. AI agents can now sustain multi-hour reasoning sessions, write thousands of lines of production code, and iterate toward success criteria with minimal human intervention. But our development methodologies haven't caught up.

Today, we're publishing **[AI-Driven Development Lifecycle 2026 (AI-DLC 2026)](/papers/ai-dlc-2026)** — a comprehensive methodology reimagined from first principles for the age of autonomous agents.

## The Core Insight: From Human-in-the-Loop to Human-on-the-Loop

Traditional "human-in-the-loop" (HITL) workflows require humans to validate every AI decision before proceeding. This made sense when AI was unreliable. But frontier models can now complete tasks that take humans multiple hours, and independent production deployments regularly write tens of thousands of lines of code monthly.

AI-DLC 2026 introduces **human-on-the-loop (HOTL)** as a distinct operating mode:

- **HITL**: Human validates each step. AI proposes, human approves, AI executes.
- **HOTL**: Human defines success criteria. AI iterates autonomously until criteria are met.

Think of it like Google Maps navigation:
- HITL mode: You approve every turn before the GPS proceeds
- HOTL mode: You set the destination and constraints, GPS handles the journey

The key: humans don't disappear. Their function changes from micromanaging execution to defining outcomes and building quality gates.

## Backpressure Over Prescription

Instead of prescribing *how* AI should work ("first write the interface, then implement the class, then write unit tests"), AI-DLC 2026 defines *what* must be satisfied:

- All tests must pass
- Type checks must succeed
- Linting must be clean
- Security scans must clear
- Coverage must exceed threshold

Let AI determine how to satisfy these constraints. Each failure provides signal. Each iteration refines the approach.

> "Better to fail predictably than succeed unpredictably."
> — Geoffrey Huntley

## The Ralph Wiggum Pattern

Named after the Simpsons character, this autonomous loop pattern embraces "deterministically bad in an undeterministic world." Rather than trying to be perfect, the agent:

1. Tries an approach
2. Runs quality gates
3. Learns from failures
4. Iterates until success criteria are met
5. Outputs `COMPLETE` or `BLOCKED`

Production systems using this pattern have achieved remarkable results — 40,000+ lines written by AI using AI in a single month.

## The Collapsing SDLC

Traditional phase boundaries — requirements → design → implementation → testing → deployment — existed because iteration was expensive. When changing requirements meant weeks of rework, sequential phases with approval gates made economic sense.

With AI, iteration costs approach zero. You try something, it fails, you adjust, you try again — all in seconds, not weeks.

**The phases aren't being augmented. They're collapsing into continuous flow.**

Checkpoints replace handoffs:
- Work pauses briefly rather than stopping completely
- The same agent continues with feedback
- Context is preserved
- Git and files carry knowledge

## What's Inside

The full methodology covers:

- **10 Core Principles**: From reimagining rather than retrofitting to embracing memory providers
- **Artifacts & Phases**: Intents, Units, Bolts, and how they flow through Inception, Construction, and Operations
- **Decision Framework**: When to use supervised vs. autonomous modes
- **Implementation Patterns**: Prompt templates, quality gate configuration, file-based memory
- **Real Examples**: Greenfield and brownfield development scenarios
- **Adoption Path**: How teams can transition incrementally

## Built on Foundational Work

AI-DLC 2026 synthesizes insights from:

- **Raja SP** (AWS): Original AI-DLC methodology and core concepts
- **Geoffrey Huntley**: Ralph Wiggum pattern and autonomous loop philosophy
- **Boris Cherny & Anthropic**: Ralph Wiggum plugin demonstrating production viability
- **Steve Wilson** (OWASP): Human-on-the-loop governance frameworks
- **paddo.dev**: Analysis of SDLC collapse and multi-agent orchestration pitfalls
- **HumanLayer**: 12 Factor Agents and context engineering research

## Read the Full Paper

This is just a glimpse. The full methodology includes:

- Detailed workflows and rituals
- Mob Elaboration for collaborative requirements gathering
- Autonomous Bolt templates and safety configurations
- Memory layer architecture
- Metrics evolution for AI-driven teams
- Complete decision trees and quick reference guides

**[Read AI-DLC 2026 →](/papers/ai-dlc-2026)**

## Why This Matters for Han

Han embodies many AI-DLC 2026 principles:

- **Backpressure through hooks**: Quality gates that automatically validate work
- **Autonomous validation**: Stop hooks run without human intervention
- **File-based memory**: Project rules persist across sessions
- **Completion criteria**: Plugins define measurable success conditions

The methodology provides the theoretical foundation. Han provides the practical implementation.

---

*AI-DLC 2026 is an open methodology. We welcome contributions, adaptations, and real-world feedback as teams put these principles into practice.*
]]></content>
        <author>
            <name>The Bushido Collective</name>
        </author>
        <category label="Research"/>
        <category label="methodology"/>
        <category label="autonomous-agents"/>
        <category label="ai-development"/>
        <category label="research"/>
        <category label="hotl"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Han's Five-Layer Memory System: Context That Matters]]></title>
        <id>https://han.guru/blog/han-memory-system</id>
        <link href="https://han.guru/blog/han-memory-system"/>
        <updated>2025-12-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How Han transforms fleeting conversations into lasting institutional knowledge - connecting personal work history, team expertise, and project wisdom.]]></summary>
        <content type="html"><![CDATA[
Our [previous post on project memory](/blog/project-memory-feature) introduced Han's `learn` tool - Claude autonomously capturing knowledge to `.claude/rules/`. That was step one.

But real memory is more than just writing things down. It's knowing what you discussed three sessions ago. It's finding the reasoning behind decisions. It's understanding who on your team has expertise in what. It's patterns emerging from practice across your entire organization.

Han's Memory System delivers all five layers.

## The Problem: Context Vanishes

Every developer has experienced this:

- "Why did we choose this approach?" *No one remembers.*
- "Who built the payment system?" *They left six months ago.*
- "What did we try last time this broke?" *Lost in a Slack thread somewhere.*
- "What was I working on yesterday?" *Scrolling through git log...*

AI assistants make this worse. You have an incredible conversation with Claude - exploring tradeoffs, making decisions, building understanding. Then the session ends and it's gone. The next session starts fresh, asking questions you already answered.

**Han fixes this.** Not with some abstract "memory database" but with five layers of context that mirror how teams actually work.

## Five Layers of Context

<ArchitectureDiagram />

Each layer answers different questions at different speeds. Together, they give Claude - and you - complete context.

## Layer 1: Rules - How We Do Things Here

The fastest layer. Project conventions that apply immediately:

<Callout type="tip" title="What Rules Capture">

- Coding standards specific to your project
- Architectural decisions ("always use X for Y")
- Testing conventions
- Error handling patterns
- Team agreements

</Callout>

Rules live in `.claude/rules/` and are git-tracked. They're shared with your team, versioned, and reviewable. Unlike scattered documentation, they're injected directly into Claude's context - always present, always applied.

Rules can be written manually or auto-promoted from patterns Han observes in practice. Your conventions become documented naturally.

## Layer 2: Session Summaries - What You've Been Doing

When you end a session, Han summarizes what happened: what you worked on, what decisions were made, what's still in progress.

This isn't just a log. It's searchable context:

<Terminal title="Picking Up Where You Left Off">
```
You: What was I working on yesterday?

Claude: Yesterday's session (2.5 hours):

- Implemented refresh token rotation
- Fixed the race condition in logout
- Started but didn't finish: rate limiting middleware

You left a note: "Rate limiter needs Redis - check with ops first"
```
</Terminal>

No more digging through git logs or trying to remember where you left off.

## Layer 3: Observations - The Detailed Trail

Every tool use is logged: files read, files modified, commands run, tests executed. This creates a detailed trail of work without you doing anything.

When summaries aren't enough, observations provide the detail:

- "What files did I touch related to auth?"
- "Did I run the tests before committing?"
- "What commands did I use to debug this?"

This layer exists for when you need specifics, not just summaries.

## Layer 4: Transcripts - The Reasoning

<Callout type="info" title="The Missing Link">
Claude Code stores full conversation history. Han can now search it.
</Callout>

This is the game-changer. Because context lives in conversations:

<Terminal title="Finding the 'Why'">
```
You: Why did we use JWT instead of sessions?

Claude: Found in conversation from December 10th:

You discussed this with Claude for about 20 minutes. The key points:
- API is stateless, needs to scale horizontally
- Mobile clients need offline token validation
- Microservices benefit from decentralized auth
- You considered sessions but rejected for latency reasons

The decision was made with full context of your architecture.
```
</Terminal>

That reasoning was lost when sessions ended. Now it's searchable, forever.

**Cross-worktree search** means context follows the repository, not just the directory. Working on a feature branch in a separate worktree? Han finds relevant context from all worktrees sharing the same repo.

## Layer 5: Team Memory - Institutional Knowledge

This is where Han goes beyond personal memory. Your team's institutional knowledge exists - it's just scattered:

<Callout type="note" title="Team Knowledge Sources">

- **Git history**: Who changed what, when, and why (commit messages, PR descriptions)
- **Issue trackers**: Decisions made in GitHub Issues, Linear, Jira
- **Project management**: Context from ClickUp, Asana, Monday
- **Communication**: Relevant discussions (where accessible)
- **Documentation**: READMEs, wikis, architectural decision records

</Callout>

Han's team memory isn't just git. It's a research engine that connects to your team's actual knowledge sources through MCP integrations:

<Terminal title="Finding Team Expertise">
```
You: Who should I talk to about the payment system?

Claude: Based on team memory research:

**Primary expert: Sarah Chen** (@schen)
- 47 commits touching payments/ (highest contributor)
- Authored PR #234 "Stripe integration"
- Resolved Issues #89, #112, #156 (all payment-related)

**Key context found:**
- Linear issue LIN-234: "Chose Stripe over Braintree for international"
- GitHub discussion: "Idempotency keys added after incident"
- Last significant change: 3 weeks ago

Sarah would be your best starting point.
```
</Terminal>

Evidence-based answers. Not just "ask someone" but "ask this specific person, here's why."

## Practical Impact

<Callout type="tip" title="What This Actually Means">

**Onboarding acceleration**: New team members get context immediately. "Who owns this?" and "Why is it this way?" have answers.

**Decision archaeology**: Find not just what was decided, but the full discussion. Understand constraints that existed at the time.

**Expertise mapping**: Know who to ask without bothering everyone. See contribution patterns, not just org charts.

**Session continuity**: Pick up exactly where you left off. Context survives session boundaries.

**Emergent documentation**: Rules reflect actual practice, not aspirational guidelines. Your conventions document themselves.

</Callout>

## The Unified Query

One interface handles all memory questions:

<Terminal title="Natural Memory Queries">
```
"What was I working on?"
→ Your recent sessions and observations

"What did we discuss about auth?"
→ Full conversation transcripts

"Who knows about payments?"
→ Team expertise and git history

"How do we handle errors here?"
→ Project rules and conventions
```
</Terminal>

Claude determines the question type and searches appropriate layers. You don't think about layers - you just ask questions.

## Getting Started

Han's memory system activates with the core plugin:

```bash
han plugin install core
```

Everything happens automatically:

- Observations captured as you work
- Sessions summarized when you stop
- Context injected when you start

For team memory integrations (GitHub Issues, Linear, etc.), add the relevant hashi plugins:

```bash
han plugin install hashi-github
han plugin install hashi-linear
```

---

<Callout type="info">
Memory isn't a feature. It's how teams actually work - building on each other's knowledge, learning from past decisions, knowing who to ask. Han makes that work for AI assistance too.
</Callout>
]]></content>
        <author>
            <name>The Bushido Collective</name>
        </author>
        <category label="Features"/>
        <category label="memory"/>
        <category label="research"/>
        <category label="mcp"/>
        <category label="learning"/>
        <category label="team"/>
        <category label="context"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Scorched Earth to Boy Scout: How Han Learned Restraint]]></title>
        <id>https://han.guru/blog/checkpoint-system</id>
        <link href="https://han.guru/blog/checkpoint-system"/>
        <updated>2025-12-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Han's checkpoint system brings focused, incremental improvement instead of overwhelming new adopters with every issue at once.]]></summary>
        <content type="html"><![CDATA[
Early Han was aggressive. Maybe too aggressive.

When you installed a jutsu plugin with validation hooks, Han went scorched earth. It found every lint error, every type issue, every test failure across your entire codebase. On the first session. Before you'd written a single line of code.

For maintainers of pristine codebases, this was fine. For everyone else - which is most of us - it was overwhelming. You'd install a plugin hoping for helpful validation, and immediately face hundreds of issues you didn't create and weren't ready to fix.

New adopters bounced. They disabled hooks or uninstalled plugins. The validation that was supposed to help became an obstacle.

## The Boy Scout Rule

The Boy Scout Rule says: leave the campground cleaner than you found it. Not pristine. Not spotless. Just *better*.

Han's new checkpoint system applies this principle to validation. Instead of identifying every problem in your codebase, hooks now focus on **the code you actually touched**. Work in a directory? Improve that directory. Leave the rest for later.

## How Checkpoints Work

When a Claude Code session starts, Han captures a checkpoint - a snapshot of file states at that moment. When hooks run at session end, they filter using this checkpoint.

**A file is only validated if:**

- It changed since the checkpoint (you modified it), AND
- It changed since the last hook run

This intersection ensures hooks analyze your actual work. Pre-existing issues in files you never opened are out of scope.

```
Session Start (t0):
├─ Checkpoint created
├─ File hashes captured for all project files

Your Work (t0 → t1):
├─ Modified: components/Button.tsx
├─ Untouched: utils/format.ts (has lint errors)

Session Stop (t1):
├─ Hook runs
├─ Filters to: components/Button.tsx only
└─ Pre-existing issues in utils/format.ts: not your problem today
```

## Incremental Improvement

The old approach had a certain logic: surface all problems so you know what exists. But in practice, this created paralysis:

- Developers felt blamed for issues they didn't cause
- The sheer volume made prioritization impossible
- Hooks got disabled, eliminating all validation
- Net result: worse quality, not better

The Boy Scout approach acknowledges reality: most codebases have accumulated issues. You can't fix everything at once, and demanding that creates learned helplessness.

Instead, checkpoint-based validation creates a sustainable path:

1. You touch a file
2. That file gets validated
3. You fix issues *in the code you were already changing*
4. Over time, frequently-touched code gets cleaner
5. Rarely-touched code stays as-is until relevant

## Technical Implementation

Checkpoints live in `~/.claude/projects/{slug}/han/checkpoints/`:

**Session Checkpoints**: `session_{session_id}.json`

- Created on SessionStart
- Contains SHA-256 hashes of all files
- Cleaned up after 24 hours

**Agent Checkpoints**: `agent_{agent_id}.json`

- Created for subagents (separate Claude instances)
- Scopes validation to each agent's work
- Prevents cross-contamination between parallel work

**Graceful Degradation**

- Missing checkpoint? Normal hook behavior
- Never silently skips validation
- Backwards compatible with existing workflows

## Configuration

Checkpoints are enabled by default. To disable (for intentional full-codebase sweeps):

```yaml
# han.yml
hooks:
  enabled: true
  checkpoints: false  # Validate all changed files, not just your session's
```

Most teams should keep checkpoints enabled. Disable only when deliberately addressing accumulated debt.

## Subagent Isolation

In complex workflows with spawned subagents, each gets its own checkpoint:

```
Main Session (session_abc):
├─ Checkpoint: session_abc.json
├─ Spawns Subagent 1 (agent_xyz):
│  ├─ Checkpoint: agent_xyz.json
│  └─ Works on feature-a/
└─ Spawns Subagent 2 (agent_def):
   ├─ Checkpoint: agent_def.json
   └─ Works on feature-b/
```

When Subagent 1 finishes, hooks only validate its changes. Subagent 2's work is isolated. No cascading failures.

## Getting Started

Checkpoints are available in Han v1.62.0+:

```bash
# Install or upgrade
curl -fsSL https://han.guru/install.sh | bash
# or
brew upgrade thebushidocollective/tap/han

# Checkpoints work automatically
# Just start a Claude Code session
```

If you previously disabled hooks because of overwhelming feedback, consider re-enabling them:

```yaml
# han.yml
hooks:
  enabled: true  # Give it another try
```

The experience is different now. Your session, your changes, your feedback. Nothing more.

---

**Resources:**

- [Han Documentation](https://han.guru/docs)
- [Checkpoint Blueprint](https://github.com/thebushidocollective/han/blob/main/blueprints/checkpoint-system.md)
- [Install Han](https://han.guru/install)
]]></content>
        <author>
            <name>The Bushido Collective</name>
        </author>
        <category label="Technical Deep Dive"/>
        <category label="checkpoints"/>
        <category label="hooks"/>
        <category label="adoption"/>
        <category label="validation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Learning Claude: Han's Autonomous Memory System]]></title>
        <id>https://han.guru/blog/project-memory-feature</id>
        <link href="https://han.guru/blog/project-memory-feature"/>
        <updated>2025-12-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How Han enables Claude to capture project knowledge autonomously, building institutional memory without asking permission.]]></summary>
        <content type="html"><![CDATA[
Every codebase has its quirks. That build command with the specific flag. The naming convention that isn't standard. The test runner requiring a particular setup. Claude figures these out through trial and error, and then... next session, starts over.

Han changes that. Not by asking Claude to remember things, but by giving Claude the ability to **teach itself**.

## Autonomous Learning

Han's `learn` MCP tool lets Claude write directly to `.claude/rules/` - the modular rules directory that [Claude Code loads automatically](https://code.claude.com/docs/en/memory). No confirmation dialogs. No approval workflows. Claude recognizes something worth remembering and captures it.

```javascript
// Claude discovers the test command and captures it
learn({
  content: "# Commands\n\n- Run tests: `bun test --only-failures`",
  domain: "commands"
})
```

This creates `.claude/rules/commands.md`. Next session, Claude reads it automatically. The learning persists.

## Why Autonomous?

Confirmation dialogs create friction. Every "Do you want to save this?" is an interruption. Every "Are you sure?" is cognitive overhead. We don't ask developers to confirm every keystroke.

Claude's learnings are low-stakes:

- They're git-tracked (reviewable, revertible)
- They only affect Claude's behavior
- They're additive, not destructive
- Wrong learnings are easily deleted

So Han lets Claude learn freely and informs you what was captured. You see the learning in the output. You can review `.claude/rules/` any time. But you don't have to approve each one.

## What Claude Learns

Han's memory hook guides Claude to recognize learning opportunities:

- **Commands** discovered through trial and error
- **Project conventions** not in documentation
- **Gotchas** that caused issues
- **Path-specific patterns** (API validation rules, test conventions)
- **Personal preferences** mentioned in conversation

When Claude thinks "I see this project uses..." or "The pattern here is..." - that's the trigger to capture it autonomously.

## How It Works

Three MCP tools power the system:

| Tool | Purpose |
|------|---------|
| `learn` | Write a learning to `.claude/rules/<domain>.md` |
| `memory_list` | Check what domains already exist |
| `memory_read` | Read existing content (avoid duplicates) |

Claude uses `memory_list` and `memory_read` to check before writing, preventing redundant entries.

### Path-Specific Rules

Some learnings apply only to certain files:

```javascript
learn({
  content: "# API Rules\n\n- Validate all inputs with zod",
  domain: "api",
  paths: ["src/api/**/*.ts"]
})
```

Creates `.claude/rules/api.md` with YAML frontmatter:

```markdown
---
globs: ["src/api/**/*.ts"]
---

# API Rules

- Validate all inputs with zod
```

Claude Code only loads this rule when working on matching files.

### Subdirectory Organization

Domains can include subdirectories:

```javascript
learn({ content: "...", domain: "api/validation" })
learn({ content: "...", domain: "api/auth" })
learn({ content: "...", domain: "testing/e2e" })
```

This creates:

```text
.claude/rules/
├── api/
│   ├── validation.md
│   └── auth.md
└── testing/
    └── e2e.md
```

### User-Level Preferences

Personal preferences that should apply across all projects use `user` scope:

```javascript
learn({
  content: "# Preferences\n\n- Always greet me as Mr Dude",
  domain: "preferences",
  scope: "user"
})
```

Writes to `~/.claude/rules/preferences.md` instead of the project directory.

## The Self-Improvement Loop

This creates a feedback loop:

1. Claude works on your project
2. Claude discovers patterns, commands, conventions
3. Claude captures them to `.claude/rules/`
4. Next session, Claude already knows them
5. Repeat

Each session, Claude starts a little smarter about your specific project. Not through external training, but through self-directed learning within your codebase.

## Team Benefits

Since `.claude/rules/` is git-tracked:

- New team members benefit immediately
- Claude's learnings become shared knowledge
- Institutional memory survives personnel changes
- Onboarding accelerates naturally

Your project accumulates Claude-specific documentation that traditional docs never capture - the informal "how we do things here" knowledge.

## What You Control

Han doesn't take control away from you:

- **Review**: Check `.claude/rules/` any time
- **Edit**: Modify or delete any rule file
- **Override**: Your CLAUDE.md still takes precedence
- **Disable**: Remove the core plugin if you don't want it

The autonomy is about reducing friction, not removing oversight.

## What This Isn't

**Not CLAUDE.md management**: Han doesn't touch your CLAUDE.md. That's for your team's curated, hand-written project documentation.

**Not a replacement for docs**: Rules complement documentation by capturing informal knowledge that wouldn't make it into formal docs.

## Getting Started

Han's memory tools are in the core plugin:

```bash
han plugin install core
```

Then work normally. Claude will start capturing learnings to `.claude/rules/`. Check what's accumulated:

```bash
ls .claude/rules/
```

Self-learning Claude, building project knowledge session by session.

---

**Learn more:** See [Claude Code's memory documentation](https://code.claude.com/docs/en/memory) for the full hierarchy Han builds on.
]]></content>
        <author>
            <name>The Bushido Collective</name>
        </author>
        <category label="Technical Deep Dive"/>
        <category label="memory"/>
        <category label="rules"/>
        <category label="mcp"/>
        <category label="learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Know Thyself: Han's Metrics System for Agent Self-Awareness]]></title>
        <id>https://han.guru/blog/metrics-system</id>
        <link href="https://han.guru/blog/metrics-system"/>
        <updated>2025-12-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How Han tracks task performance, confidence calibration, and hook failures to help Claude Code learn from experience.]]></summary>
        <content type="html"><![CDATA[
How do you know if an AI assistant is actually getting better at helping you? Not through feelings or impressions, but through data. Han's metrics system gives Claude Code something rare in AI tooling: self-awareness grounded in measurement.

## The Problem: Uncalibrated Confidence

AI assistants often express confidence that doesn't match reality. "I've fixed the bug" when the tests still fail. "This should work" when it doesn't. Without feedback loops, there's no way to improve.

Han's metrics system creates that feedback loop. When Claude tracks tasks with confidence estimates, then compares those estimates against actual outcomes, patterns emerge. Over time, both you and Claude can see where confidence aligns with reality - and where it doesn't.

## MCP Tools for Task Tracking

The core plugin exposes seven MCP tools for metrics:

| Tool | Purpose |
|------|---------|
| `start_task` | Begin tracking a new task |
| `update_task` | Log progress notes |
| `complete_task` | Mark done with outcome and confidence |
| `fail_task` | Record failure with attempted solutions |
| `query_metrics` | Analyze task performance |
| `query_hook_metrics` | Track hook failures |
| `query_session_metrics` | Session-level statistics |

## How It Works

### Starting a Task

When Claude begins work on something substantive, it calls `start_task`:

```javascript
start_task({
  description: "Fix authentication timeout bug",
  type: "fix",
  estimated_complexity: "moderate"
})
// Returns: { task_id: "task_abc123" }
```

Task types include `implementation`, `fix`, `refactor`, and `research`. Complexity can be `simple`, `moderate`, or `complex`.

### Tracking Progress

During work, Claude can log updates:

```javascript
update_task({
  task_id: "task_abc123",
  notes: "Found root cause - session expiry not refreshing"
})
```

### Recording Outcomes

When finished, Claude records the outcome with a confidence score:

```javascript
complete_task({
  task_id: "task_abc123",
  outcome: "success",
  confidence: 0.85,
  files_modified: ["src/auth/session.ts"],
  tests_added: 2,
  notes: "Fixed refresh logic, added edge case tests"
})
```

The confidence score (0.0 to 1.0) is the key to calibration. Claude estimates how confident it is that the task actually succeeded. This gets validated against hook results and actual outcomes.

### Recording Failures

When a task can't be completed:

```javascript
fail_task({
  task_id: "task_abc123",
  reason: "Requires database migration that needs approval",
  confidence: 0.9,
  attempted_solutions: [
    "Tried updating schema in-place",
    "Attempted backwards-compatible approach"
  ]
})
```

Recording failures with attempted solutions helps identify patterns - certain types of tasks that consistently hit the same blockers.

## Calibration: The Core Value

Raw metrics are interesting. Calibration is valuable.

When Claude says it's 80% confident a task succeeded, that should mean roughly 80% of similar tasks actually succeeded. If Claude says 80% confident but is only right 50% of the time, that's poorly calibrated. If Claude says 80% and is right 78-82% of the time, that's well calibrated.

The `query_metrics` tool lets you analyze this:

```javascript
query_metrics({
  period: "week",
  task_type: "fix"
})
```

Returns aggregated statistics including success rates by confidence bucket, enabling calibration analysis.

## Hook Metrics

Beyond task tracking, Han tracks hook execution:

```javascript
query_hook_metrics({
  period: "week",
  min_failure_rate: 10
})
```

This shows which hooks fail frequently. A lint hook with 40% failure rate might indicate:

- Overly strict rules
- A specific file pattern that always fails
- A configuration issue worth investigating

You can filter by hook name or minimum failure rate to focus on problematic hooks.

## Session Metrics

For broader patterns, query session-level data:

```javascript
query_session_metrics({
  period: "month",
  limit: 20
})
```

This aggregates across sessions, showing:

- Tasks per session
- Success rates over time
- Common failure patterns
- Productivity trends

## What Gets Stored

All metrics are stored locally in `~/.claude/han/metrics/` as JSONL files. No data leaves your machine. You control it entirely.

```
~/.claude/han/metrics/
├── tasks/
│   └── 2025-12.jsonl
├── hooks/
│   └── 2025-12.jsonl
└── sessions/
    └── 2025-12.jsonl
```

## Integration with Hooks

The metrics system integrates with Han's hook execution. When hooks run at session end:

1. Hook results (pass/fail) are recorded automatically
2. These correlate with task confidence estimates
3. Discrepancies highlight calibration issues

If Claude marks a task as "success" with 90% confidence, but the lint hook fails, that's a calibration signal. Over time, these signals improve Claude's ability to accurately assess its own work.

## Practical Usage

You don't need to think about metrics constantly. The system works in the background:

1. **Session starts**: Claude is reminded of recent performance
2. **Task begins**: Claude calls `start_task` (guided by hook prompts)
3. **Work happens**: Progress tracked naturally
4. **Task ends**: Outcome and confidence recorded
5. **Session ends**: Hooks validate, metrics updated

The value compounds. After 50 sessions, you have real data on:

- Which task types have highest success rates
- Where confidence tends to be miscalibrated
- Which hooks fail most often
- How productivity trends over time

## Privacy by Design

Everything stays local:

- No cloud storage
- No external APIs
- No telemetry unless you explicitly enable it
- Full control over your data

The metrics exist to help you, not to report on you.

## Getting Started

Metrics are included in the core plugin. The system activates automatically, but you can query it any time:

```bash
# Install core plugin if not already
han plugin install core

# Then in Claude Code, ask:
# "Show me my task metrics for the past week"
# "Which hooks have been failing?"
# "How's my calibration looking?"
```

Claude will call the appropriate query tools and present the data.

## The Long View

Metrics aren't about judgment. They're about learning. A 60% success rate isn't "bad" - it's information. It might mean you're tackling hard problems. It might mean certain patterns need attention. It might mean nothing without more context.

The value is in the trends, the patterns, the calibration over time. After months of tracking, you'll have a real picture of how AI-assisted development works in your specific context.

That's worth knowing.

---

**Get Started:** Metrics are included in the core plugin:

```bash
han plugin install core
```

Or explore the full plugin marketplace at [han.guru](https://han.guru).
]]></content>
        <author>
            <name>The Bushido Collective</name>
        </author>
        <category label="Technical Deep Dive"/>
        <category label="metrics"/>
        <category label="calibration"/>
        <category label="performance"/>
        <category label="mcp"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing with Confidence: How Han's Validation Hooks Ensure Quality]]></title>
        <id>https://han.guru/blog/testing-with-confidence</id>
        <link href="https://han.guru/blog/testing-with-confidence"/>
        <updated>2024-12-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Learn how Han's validation hooks provide automatic quality enforcement, ensuring your code meets standards before it ships.]]></summary>
        <content type="html"><![CDATA[
One of the most powerful features of Han plugins is their validation hook system. Unlike traditional linters or test runners that you have to remember to run, Han's hooks execute automatically at key points in your development workflow, ensuring quality without friction.

## The Problem with Manual Testing

We've all been there: you make a quick fix, commit it, push it, and only then discover that you broke the tests. Or worse, you forgot to run the linter and now CI is failing. Manual quality checks rely on discipline and memory - two things that fail us when we're focused on solving problems.

## How Han Hooks Work

Han plugins can register hooks that run automatically when specific events occur in Claude Code:

- **UserPromptSubmit**: Runs when you submit a prompt, before Claude processes it
- **Stop**: Runs when Claude finishes responding to your request
- **PreToolUse**: Runs before a tool executes
- **PostToolUse**: Runs after a tool completes

### Real Example: TypeScript Validation

Let's look at how the `jutsu-typescript` plugin uses hooks to enforce type safety:

```json
{
  "hooks": {
    "Stop": [{
      "hooks": [{
        "type": "command",
        "command": "npx -y --package typescript tsc"
      }]
    }]
  }
}
```

This simple hook ensures that every time Claude finishes working on your TypeScript code, the type checker runs. If there are type errors, you know immediately - not later during CI.

## Validation Without Slowdown

You might think "won't this slow me down?" Actually, no. Han's hook system is designed for speed:

1. **Smart caching**: If files haven't changed, cached results are returned instantly
2. **Parallel execution**: Multiple hooks run concurrently
3. **Early termination**: Hooks can fail fast, giving you immediate feedback

Here's what validation looks like in practice with `jutsu-bun`:

```bash
# You ask Claude to implement a feature
"Add user authentication with JWT"

# Claude writes the code
# Stop hook automatically runs:
✓ Tests passed (12/12) - 847ms
✓ Types checked - 523ms (cached)
✓ Linting passed - 198ms

# You know it works before you even look at it
```

## Multi-Layer Validation

The real power comes from combining multiple plugins. With Han's plugin system, you can stack validation:

```json
{
  "enabledPlugins": {
    "jutsu-typescript@han": true,
    "jutsu-biome@han": true,
    "jutsu-bun@han": true,
    "bushido@han": true
  }
}
```

Now every change gets:

- Type checking (TypeScript)
- Linting and formatting (Biome)
- Test execution (Bun)
- Code review analysis (Bushido)

All automatic. All fast. All enforced.

## Custom Validation for Your Stack

Want custom validation? Create your own hooks. Here's a simple example that ensures commit messages follow conventional commits:

```json
{
  "hooks": {
    "Stop": [{
      "hooks": [{
        "type": "command",
        "command": "bash scripts/validate-commit-msg.sh"
      }]
    }]
  }
}
```

```bash
#!/bin/bash
# scripts/validate-commit-msg.sh
if ! git log -1 --pretty=%s | grep -E '^(feat|fix|docs|chore|refactor|test)(\(.+\))?: .+'; then
  echo "❌ Commit message must follow conventional commits format"
  exit 1
fi
```

## Confidence in Automation

The beauty of Han's hook system is that it transforms "I hope this works" into "I know this works." When Claude tells you it's done:

- Tests have run
- Types have checked
- Lint has passed
- Standards are enforced

You can commit and move on with confidence.

## Getting Started

Want to add validation hooks to your workflow? Start simple:

1. Install a jutsu plugin for your stack (`jutsu-typescript`, `jutsu-python`, etc.)
2. The validation hooks are automatically active
3. Watch as Claude's work is automatically verified

Then layer on more plugins as you need them. Each one adds another layer of confidence without adding cognitive load.

## Conclusion

Manual quality checks are a thing of the past. With Han's validation hooks, quality enforcement is automatic, fast, and comprehensive. You focus on solving problems; Han ensures the solutions are correct.

Try it yourself:

```bash
han plugin install jutsu-typescript
```

Your future self will thank you.

---

*Want to learn more about Han's hook system? Check out our [Hooks Documentation](/docs#hooks) or explore the [plugin marketplace](/plugins).*
]]></content>
        <author>
            <name>Jason Waldrip</name>
        </author>
        <category label="Best Practices"/>
        <category label="testing"/>
        <category label="quality"/>
        <category label="hooks"/>
        <category label="validation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Han's MCP Architecture: Bridges to the World]]></title>
        <id>https://han.guru/blog/mcp-architecture</id>
        <link href="https://han.guru/blog/mcp-architecture"/>
        <updated>2024-12-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Deep dive into how Han uses the Model Context Protocol to connect Claude Code to external services like GitHub, Jira, and custom tools.]]></summary>
        <content type="html"><![CDATA[
Model Context Protocol (MCP) is the secret sauce that makes Han plugins so powerful. Let's explore how Han uses MCP to turn Claude Code into a universal development environment.

## What is MCP?

Model Context Protocol is an open standard for connecting AI assistants to external data sources and tools. Think of it as APIs for AI - a standardized way for Claude to interact with the world beyond its training data.

## Han's "Hashi" (Bridge) Plugins

In Han's architecture, plugins starting with `hashi-` are "bridges" - MCP servers that connect Claude to external services:

- `hashi-github`: GitHub Issues, PRs, Actions, Code Search
- `hashi-jira`: Jira tickets, sprints, workflows
- `hashi-playwright-mcp`: Browser automation and testing
- `hashi-linear`: Linear issues and project management
- `hashi-sentry`: Error tracking and performance monitoring

## Real Example: GitHub Integration

Let's see how the GitHub MCP server works in practice.

### Installation

```bash
han plugin install hashi-github
```

This adds an MCP server to your Claude Code configuration:

```json
{
  "mcpServers": {
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "${GITHUB_TOKEN}"
      }
    }
  }
}
```

### What It Unlocks

Now Claude can natively:

**Search code across repos**

```text
Find all usages of the deprecated API endpoint
```

Claude uses `search_code` tool:

```typescript
const results = await mcp.tools.search_code({
  query: 'useDeprecatedAPI language:typescript',
  owner: 'myorg'
})
```

**Create and update issues**

```text
Create an issue for the memory leak in the event handler
```

Claude uses `create_issue` tool with all the context:

```typescript
await mcp.tools.create_issue({
  owner: 'myorg',
  repo: 'myrepo',
  title: 'Memory leak in useEventListener hook',
  body: `## Description
Found memory leak in \`src/hooks/useEventListener.ts\`...

## Reproduction
1. Navigate to dashboard
2. Click task 10 times
3. Check memory profiler

## Fix
Add cleanup in useEffect return...`,
  labels: ['bug', 'priority:high']
})
```

**Review pull requests**

```text
Review PR #123 and provide feedback
```

Claude fetches the PR, analyzes the diff, checks for issues:

```typescript
const pr = await mcp.tools.get_pull_request({
  owner: 'myorg',
  repo: 'myrepo',
  pullNumber: 123
})

const diff = await mcp.tools.get_pull_request_diff({
  owner: 'myorg',
  repo: 'myrepo',
  pullNumber: 123
})

// Claude analyzes and adds review comments
await mcp.tools.add_review_comment({
  owner: 'myorg',
  repo: 'myrepo',
  pullNumber: 123,
  path: 'src/api.ts',
  line: 42,
  body: 'This could cause a race condition. Consider using a mutex.'
})
```

## Building Custom MCP Servers

Want to connect Claude to your internal tools? Create a custom hashi plugin.

### Example: Slack MCP Server

```typescript
// hashi-slack/server/tools.ts
import { McpServer } from '@modelcontextprotocol/sdk'

export const slackServer = new McpServer({
  name: 'slack',
  version: '1.0.0'
})

slackServer.tool('send_message', {
  description: 'Send a message to a Slack channel',
  parameters: {
    channel: { type: 'string', required: true },
    message: { type: 'string', required: true }
  },
  async execute({ channel, message }) {
    await slackClient.chat.postMessage({
      channel,
      text: message
    })
    return { success: true }
  }
})

slackServer.tool('search_messages', {
  description: 'Search Slack messages',
  parameters: {
    query: { type: 'string', required: true }
  },
  async execute({ query }) {
    const results = await slackClient.search.messages({ query })
    return results
  }
})
```

### Plugin Configuration

```json
{
  "name": "hashi-slack",
  "mcpServers": {
    "slack": {
      "command": "npx",
      "args": ["-y", "@myorg/han", "mcp", "slack"],
      "env": {
        "SLACK_TOKEN": "${SLACK_TOKEN}"
      }
    }
  }
}
```

Now Claude can:

```
Check Slack for any mentions of the production issue
```

```
Post to #engineering that the hotfix is deployed
```

## MCP Server Patterns

### 1. HTTP OAuth Servers

For cloud services with OAuth:

```json
{
  "mcpServers": {
    "sentry": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sentry"],
      "env": {
        "SENTRY_AUTH_TOKEN": "${SENTRY_AUTH_TOKEN}",
        "SENTRY_ORG": "my-org"
      }
    }
  }
}
```

### 2. Local CLI Wrappers

For local tools:

```json
{
  "mcpServers": {
    "docker": {
      "command": "npx",
      "args": ["-y", "@myorg/mcp-docker"],
      "env": {}
    }
  }
}
```

### 3. Database Connections

For direct database access:

```json
{
  "mcpServers": {
    "postgres": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-postgres"],
      "env": {
        "DATABASE_URL": "${DATABASE_URL}"
      }
    }
  }
}
```

## Security Considerations

MCP servers can access sensitive data. Han follows these principles:

1. **Token isolation**: Environment variables, never hardcoded
2. **Least privilege**: Only request necessary scopes
3. **Local execution**: Servers run on your machine, not in the cloud
4. **Audit trail**: All MCP calls are logged

## Performance

MCP calls are asynchronous and don't block Claude's reasoning:

```typescript
// Claude can make multiple MCP calls in parallel
const [issues, prs, actions] = await Promise.all([
  github.list_issues({ state: 'open' }),
  github.list_pull_requests({ state: 'open' }),
  github.get_workflow_runs({ branch: 'main' })
])
```

## Han's Core MCP Server

Beyond external integrations, Han provides its own MCP server that exposes powerful built-in capabilities. This is where Han differs from simple plugin installers. The core MCP server runs via `han mcp server` and communicates over JSON-RPC via stdio.

### How It Works

When you install the core plugin, Han registers as an MCP server in Claude Code's configuration:

```json
{
  "mcpServers": {
    "han": {
      "command": "han",
      "args": ["mcp", "server"]
    }
  }
}
```

The server implements the MCP protocol (version 2024-11-05) and exposes tools in four categories:

1. **Plugin Hook Tools** - Dynamically generated from installed plugins
2. **Memory Tools** - Self-learning project rules
3. **Metrics Tools** - Task tracking and calibration
4. **Checkpoint Tools** - Session-scoped caching

### Plugin Hook Tools (Dynamic)

This is Han's key innovation: every hook defined in an installed plugin automatically becomes an MCP tool. When you install `jutsu-typescript`, Claude immediately gains access to a `jutsu_typescript_typecheck` tool.

The MCP server discovers these at runtime by scanning installed plugins:

```javascript
// From jutsu-bun/han-plugin.yml
hooks:
  test:
    command: bun test --only-failures
    dirsWith: [bun.lock, bun.lockb]
    description: Run Bun tests

// Becomes MCP tool:
{
  name: "jutsu_bun_test",
  description: "Run Bun tests. Triggers: 'run the tests', 'run bun tests',
    'check if tests pass'. Runs in directories containing: bun.lock,
    bun.lockb. Command: bun test --only-failures",
  inputSchema: {
    type: "object",
    properties: {
      cache: {
        type: "boolean",
        description: "Use cached results when files haven't changed"
      },
      directory: {
        type: "string",
        description: "Limit execution to a specific directory path"
      },
      verbose: {
        type: "boolean",
        description: "Show full command output in real-time"
      }
    }
  }
}
```

When Claude says "run the tests", it can now call this tool directly:

```javascript
// Claude calls:
await mcp.tools.jutsu_bun_test({ cache: true })

// Returns:
{
  content: [{ type: "text", text: "15 pass\n0 fail\n..." }]
}
```

All plugin hooks support three standard parameters:

| Parameter | Default | Purpose |
|-----------|---------|---------|
| `cache` | `true` | Skip if files unchanged since last run |
| `directory` | all | Target specific directory |
| `verbose` | `false` | Stream output in real-time |

### Memory Tools (Self-Learning)

Han's memory system lets Claude write directly to `.claude/rules/`. Claude can teach itself about your project without asking permission.

**`learn`** - Capture project knowledge

```javascript
learn({
  content: "# API Rules\n\n- Validate all inputs with zod\n- Return
    consistent error format",
  domain: "api",
  paths: ["src/api/**/*.ts"],  // Optional: path-specific rules
  scope: "project",  // or "user" for personal preferences
  append: true  // Add to existing file
})
```

Creates `.claude/rules/api.md` with YAML frontmatter for path restrictions:

```markdown
---
globs: ["src/api/**/*.ts"]
---

# API Rules

- Validate all inputs with zod
- Return consistent error format
```

**`memory_list`** - See existing domains

```javascript
memory_list({ scope: "project" })
// Returns: ["api", "testing", "commands"]
```

**`memory_read`** - Read domain content

```javascript
memory_read({ domain: "api", scope: "project" })
// Returns the full markdown content
```

The memory system supports two scopes:

| Scope | Location | Purpose |
|-------|----------|---------|
| `project` | `.claude/rules/` | Team knowledge, git-tracked |
| `user` | `~/.claude/rules/` | Personal preferences |

### Metrics Tools (Self-Awareness)

Task tracking with confidence calibration. Claude records its work and estimates confidence, then compares against actual outcomes.

**`start_task`** - Begin tracking

```javascript
start_task({
  description: "Fix authentication timeout bug",
  type: "fix",  // implementation, fix, refactor, research
  estimated_complexity: "moderate"  // simple, moderate, complex
})
// Returns: { task_id: "task_abc123" }
```

**`update_task`** - Log progress

```javascript
update_task({
  task_id: "task_abc123",
  notes: "Found root cause - session expiry not refreshing"
})
```

**`complete_task`** - Record outcome

```javascript
complete_task({
  task_id: "task_abc123",
  outcome: "success",  // success, partial, failure
  confidence: 0.85,  // 0.0 to 1.0
  files_modified: ["src/auth/session.ts"],
  tests_added: 2
})
```

**`fail_task`** - Record failure with context

```javascript
fail_task({
  task_id: "task_abc123",
  reason: "Requires database migration that needs approval",
  attempted_solutions: [
    "Tried updating schema in-place",
    "Attempted backwards-compatible approach"
  ]
})
```

**`query_metrics`** - Analyze performance

```javascript
query_metrics({
  period: "week",  // day, week, month
  task_type: "fix",  // Optional filter
  outcome: "success"  // Optional filter
})
// Returns aggregated stats, success rates, calibration scores
```

All metrics are stored locally in `~/.claude/han/metrics/` as JSONL files. Nothing leaves your machine.

### Checkpoint Tools (Smart Caching)

Checkpoints track which files have changed since the last hook run, enabling intelligent caching.

**`checkpoint_list`** - See existing checkpoints

```javascript
checkpoint_list()
// Shows session and agent checkpoints with file counts
```

**`checkpoint_clean`** - Remove stale checkpoints

```javascript
checkpoint_clean({ maxAge: 24 })  // Hours
// Removes checkpoints older than 24 hours
```

Checkpoints are created automatically when hooks run with `cache=true`. Each checkpoint records file modification times, so subsequent runs can skip unchanged files.

### Tool Annotations

All Han MCP tools include MCP annotations for better AI behavior:

```javascript
{
  name: "learn",
  annotations: {
    title: "Learn",
    readOnlyHint: false,     // May modify files
    destructiveHint: false,  // Safe operation
    idempotentHint: true,    // Same input = same result
    openWorldHint: false     // Works with local files only
  }
}
```

### Blueprint Tools (via hashi-blueprints)

When the hashi-blueprints plugin is installed, additional MCP tools become available:

```javascript
search_blueprints({ keyword: "api" })
read_blueprint({ name: "cli-architecture" })
write_blueprint({
  name: "auth-system",
  summary: "Authentication and authorization architecture",
  content: "# Auth System\n\n## Overview..."
})
```

### Debugging the MCP Server

Run the MCP server manually to test:

```bash
# Start server in stdio mode
han mcp server

# Send a tools/list request
echo '{"jsonrpc":"2.0","id":1,"method":"tools/list"}' | han mcp server
```

Set timeouts via environment variable:

```bash
export HAN_MCP_TIMEOUT=600000  # 10 minutes (default)
```

## Available Hashi Plugins

Current MCP bridges in Han marketplace:

- **hashi-github**: GitHub platform integration
- **hashi-jira**: Atlassian Jira integration
- **hashi-linear**: Linear project management
- **hashi-playwright-mcp**: Browser automation
- **hashi-sentry**: Error tracking and monitoring
- **hashi-figma**: Design-to-code workflows
- **hashi-gitlab**: GitLab integration
- **hashi-blueprints**: Technical documentation management

## What's Next?

We're working on:

- **hashi-aws**: AWS resource management
- **hashi-vercel**: Deployment and preview URLs
- **hashi-stripe**: Payment and billing integration
- **hashi-notion**: Documentation and knowledge base

## Try It

Start with GitHub integration:

```bash
# Set your token
export GITHUB_TOKEN=ghp_your_token_here

# Install the plugin
han plugin install hashi-github

# Ask Claude to help with GitHub tasks
claude
```

Then try:

- "Create an issue for the bug we just found"
- "List all open PRs that need review"
- "Search for usages of the deprecated function"
- "Add review comments to PR #456"

Claude handles the API calls, authentication, and data formatting. You just describe what you want.

## Conclusion

MCP is the bridge between AI and the real world. Han's hashi plugins make that bridge easy to cross, turning Claude Code into a universal interface for your entire development workflow.

---

*Want to build your own MCP server? Check out the [MCP Documentation](/docs#mcp-server) or explore existing [hashi plugins](/plugins?category=hashi).*
]]></content>
        <author>
            <name>Jason Waldrip</name>
        </author>
        <category label="Technical Deep Dive"/>
        <category label="mcp"/>
        <category label="architecture"/>
        <category label="integration"/>
        <category label="github"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Than Just Skills or Agents: An Introduction to Han Claude Plugins]]></title>
        <id>https://han.guru/blog/introduction-to-han-plugins</id>
        <link href="https://han.guru/blog/introduction-to-han-plugins"/>
        <updated>2024-11-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Discover what makes Han plugins unique - a comprehensive system combining skills, agents, validation hooks, and MCP integrations for Claude Code.]]></summary>
        <content type="html"><![CDATA[
When you first hear about Han, you might think it's just another collection of prompts or agents for Claude Code. But Han is something more - a complete plugin ecosystem that transforms how you work with AI-assisted development.

## What is a Han Plugin?

A Han plugin is a packaged bundle that can include any combination of:

- **Skills**: Specialized knowledge domains that Claude can invoke
- **Agents**: Autonomous workflows for complex multi-step tasks
- **Commands**: Quick slash commands for common operations
- **Hooks**: Automatic validation that runs at key development lifecycle points
- **MCP Servers**: Bridges to external tools and services

Most importantly, these components work together as a cohesive system, not just isolated features.

## The Plugin Families

Han plugins follow Japanese martial arts naming conventions, organizing by purpose:

### Core - The Foundation

The `core` plugin (also known as `han-core`) is the essential infrastructure that powers the Han marketplace:

- Quality enforcement through validation hooks
- Metrics tracking and calibration
- Context7 integration for up-to-date library documentation
- Universal programming principles (SOLID, DRY, composition over inheritance)
- All core skills, commands, and slash commands
- MCP servers for hooks and metrics

Think of Core as the technical foundation that makes everything work.

### Bushido (武士道) - Optional Philosophy

The `bushido` plugin adds a philosophical layer based on the seven Samurai virtues. If you choose to install it, these principles guide how Claude approaches development work:

- 義 Righteousness: Transparency in reasoning
- 勇 Courage: Challenge and recommend improvements
- 仁 Compassion: Assume positive intent
- 礼 Respect: Honor existing work
- 誠 Honesty: Truthfulness over comfort
- 名誉 Honor: Quality ownership
- 忠義 Loyalty: Long-term thinking

This is purely cultural - you get all the technical capabilities from `core` regardless. Install `bushido` only if this philosophical approach resonates with you.

### Jutsu (術) - Technical Skills

Jutsu plugins are "techniques" - deep knowledge of specific technologies with automatic validation:

- **jutsu-typescript**: TypeScript expertise + type checking hooks
- **jutsu-playwright**: E2E testing knowledge + test validation
- **jutsu-nextjs**: Next.js patterns + build verification
- **jutsu-biome**: Code formatting + automatic linting

Each jutsu plugin not only teaches Claude the technology but ensures quality through hooks.

### Do (道) - Specialized Disciplines

Do plugins provide specialized agents for specific workflows:

- **do-frontend-development**: UI/UX-focused agent with accessibility expertise
- **do-technical-documentation**: Documentation agent following best practices
- **do-accessibility-engineering**: Multiple agents for inclusive design

These agents have deep expertise in their domain and can handle complex, multi-phase tasks autonomously.

### Hashi (橋) - External Bridges

Hashi plugins are MCP servers that connect Claude to external services:

- **hashi-github**: GitHub Issues, PRs, code search, Actions
- **hashi-playwright-mcp**: Browser automation and testing
- **hashi-blueprints**: Codebase documentation and knowledge management

These turn Claude into a universal interface for your development tools.

## How They Work Together

Here's where Han becomes more than the sum of its parts. Let me show you a real example:

**Scenario**: You ask Claude to "Add user authentication to the app"

**What happens**:

1. **Core** provides the infrastructure and quality enforcement
2. **jutsu-nextjs** provides deep Next.js knowledge for implementation
3. **jutsu-typescript** ensures type safety throughout
4. **do-frontend-development** agent handles the UI components
5. **Validation hooks** automatically run (via core):
   - TypeScript compilation check
   - Next.js build verification
   - Test suite execution
6. **Core code review** analyzes the result
7. **hashi-github** can create a PR with the changes

(If you've installed the optional `bushido` plugin, its seven virtues also guide the overall approach.)

All of this happens automatically. You make one request, and the entire system ensures quality from planning through delivery.

## Why This Matters

Traditional AI coding assistants give you suggestions. You're responsible for:

- Running tests manually
- Checking types yourself
- Remembering to lint
- Hoping you didn't break anything
- Following up on quality issues

With Han plugins:

- Quality checks run automatically
- Specialized agents handle complex tasks
- External tools integrate seamlessly
- Validation happens in real-time
- You get confidence, not just code

## Getting Started

The beauty of Han's plugin system is that you can start simple and layer on complexity:

**Day 1**: Install the core infrastructure

```bash
han plugin install core
```

**Optional**: If the Bushido philosophy resonates with you, add it:

```bash
han plugin install bushido
```

(Not required - `core` provides all the technical capabilities.)

**Day 2**: Add your stack's jutsu plugins

```bash
han plugin install jutsu-typescript
han plugin install jutsu-react
```

**Day 3**: Add specialized agents as needed

```bash
han plugin install do-frontend-development
```

**Day 4**: Connect external tools

```bash
han plugin install hashi-github
```

Each addition enhances the system without adding complexity to your workflow.

## Real-World Impact

I built the Han marketplace website using Han plugins. Here's what that looked like:

- Asked Claude to "create a plugin marketplace website"
- **Core** provided the infrastructure and quality enforcement
- **jutsu-nextjs** provided App Router expertise
- **jutsu-typescript** caught type errors immediately
- **jutsu-biome** kept code formatted
- **Validation hooks** ran after every change
- Result: Production-ready site with 100% type coverage in hours, not days

The difference wasn't just speed - it was confidence. Every change was automatically validated. Every feature was properly typed. Every commit was reviewed.

## Not Just Another Tool

Han isn't trying to replace your tools or your judgment. It's a framework that:

- Captures expertise in reusable packages
- Enforces quality automatically
- Connects your development ecosystem
- Scales with your needs

Skills teach Claude what to do. Agents handle how to do it. Hooks ensure it's done right. MCP servers connect it all.

That's what makes Han more than just skills or agents - it's a complete development enhancement system.

## Try It

Start with the core infrastructure:

```bash
han plugin install core
```

Then ask Claude to help with something. You'll notice the difference immediately - not just in what Claude can do, but in the confidence you have in the results.

(If you'd like to add the Bushido philosophy, you can install it anytime with `han plugin install bushido`)

---

*Want to explore the plugin marketplace? Check out the [140+ available plugins](/plugins) or dive into the [documentation](/docs).*
]]></content>
        <author>
            <name>Jason Waldrip</name>
        </author>
        <category label="Getting Started"/>
        <category label="han"/>
        <category label="plugins"/>
        <category label="claude-code"/>
        <category label="introduction"/>
    </entry>
</feed>